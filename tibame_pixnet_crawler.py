# -*- coding: utf-8 -*-
"""Tibame專題.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LiFxLlxUo5a0LOPyFwfbixHh9Hv1pxg_

# 載入必要套件
"""

!pip install requests beautifulsoup4

import requests
from bs4 import BeautifulSoup

url = "https://paicj.pixnet.net/blog/post/407084009"
headers = {
    "User-Agent": "Mozilla/5.0"
}
response = requests.get(url, headers=headers)
response.encoding = 'utf-8'

soup = BeautifulSoup(response.text, 'html.parser')

title_tag = soup.find('h2', class_='article-title')
title = title_tag.get_text(strip=True) if title_tag else '找不到標題'

for h2 in soup.find_all('h2'):
    a_tag = h2.find('a')
    if a_tag and a_tag.has_attr('href'):
        print("標題：", a_tag.get_text(strip=True))
        print("連結：", a_tag['href'])

content_div = soup.find('div', class_='article-content-inner')
if content_div:
    paragraphs = content_div.find_all('p')
    content = '\n'.join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))
else:
    content = '找不到內容'


print("\n內容：\n", content)

import requests
from bs4 import BeautifulSoup

# 1. 發送 GET 請求
url = "https://paicj.pixnet.net/blog/post/406882969"
headers = {
    "User-Agent": "Mozilla/5.0"
}
response = requests.get(url, headers=headers)
response.encoding = 'utf-8'

# 2. 解析 HTML
soup = BeautifulSoup(response.text, 'html.parser')

# 3. 擷取標題
title_tag = soup.find('h2', class_='article-title')
title = title_tag.get_text(strip=True) if title_tag else '找不到標題'

for h2 in soup.find_all('h2'):
    a_tag = h2.find('a')
    if a_tag and a_tag.has_attr('href'):
        print("標題：", a_tag.get_text(strip=True))
        print("連結：", a_tag['href'])

# 4. 擷取內容
content_div = soup.find('div', class_='article-content-inner')
if content_div:
    paragraphs = content_div.find_all('p')
    content = '\n'.join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))
else:
    content = '找不到內容'

# 5. 顯示結果
print("\n內容：\n", content)

import requests
from bs4 import BeautifulSoup

# 1. 發送 GET 請求
url = "https://paicj.pixnet.net/blog/post/406882969"
headers = {
    "User-Agent": "Mozilla/5.0"
}
response = requests.get(url, headers=headers)
response.encoding = 'utf-8'

# 2. 解析 HTML
soup = BeautifulSoup(response.text, 'html.parser')

# 3. 擷取標題
title_tag = soup.find('h2', class_='article-title')
title = title_tag.get_text(strip=True) if title_tag else '找不到標題'

for h2 in soup.find_all('h2'):
    a_tag = h2.find('a')
    if a_tag and a_tag.has_attr('href'):
        print("標題：", a_tag.get_text(strip=True))
        print("連結：", a_tag['href'])

# 4. 擷取內容
content_div = soup.find('div', class_='article-content-inner')
if content_div:
    paragraphs = content_div.find_all('p')
    content = '\n'.join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))
else:
    content = '找不到內容'

# 5. 顯示結果
print("\n內容：\n", content)

import requests
from bs4 import BeautifulSoup

# Medium 文章 URL
url = "https://medium.com/旅行之地/動機-dbe90d4d737c"

# 設定 headers 模擬瀏覽器行為
headers = {
    "User-Agent": "Mozilla/5.0"
}

# 發送 GET 請求
response = requests.get(url, headers=headers)

# 檢查請求是否成功
if response.status_code == 200:
    # 解析 HTML 內容
    soup = BeautifulSoup(response.text, "html.parser")

    # 嘗試抓取文章標題
    title_tag = soup.find("h1")
    title = title_tag.get_text(strip=True) if title_tag else "無標題"

    # 嘗試抓取文章內容
    paragraphs = soup.find_all("p")
    content = "\n".join(p.get_text(strip=True) for p in paragraphs)

    # 輸出結果
    print(f"標題：{title}\n")
    print("內容：")
    print(content)
else:
    print(f"請求失敗，狀態碼：{response.status_code}")

import requests
from bs4 import BeautifulSoup

# Medium 文章 URL
url = "https://medium.com/@angela-ye-zhijunye/zjs-nature-diary-1-let-s-go-camping-%E8%8B%97%E6%A0%97%E9%9C%B2%E7%87%9F%E5%8D%80-%E5%8F%B8%E9%A6%AC%E9%99%90%E9%9B%B2%E7%AB%AF-%E5%B9%B3%E6%97%A5%E5%B0%8F%E7%A2%BA%E5%B9%B8-%E7%AC%AC%E4%B8%80%E6%AC%A1%E5%B0%B1%E6%88%90%E5%8A%9F%E7%9A%84%E9%9B%99%E4%BA%BA%E9%9C%B2%E7%87%9F-873cbd38d5b6"

# 設定 headers 模擬瀏覽器行為
headers = {
    "User-Agent": "Mozilla/5.0"
}

# 發送 GET 請求
response = requests.get(url, headers=headers)

# 檢查請求是否成功
if response.status_code == 200:
    # 解析 HTML 內容
    soup = BeautifulSoup(response.text, "html.parser")

    # 嘗試抓取文章標題
    title_tag = soup.find("h1")
    title = title_tag.get_text(strip=True) if title_tag else "無標題"

    # 嘗試抓取文章內容
    paragraphs = soup.find_all("p")
    content = "\n".join(p.get_text(strip=True) for p in paragraphs)

    # 輸出結果
    print(f"標題：{title}\n")
    print("內容：")
    print(content)
else:
    print(f"請求失敗，狀態碼：{response.status_code}")

import requests
from bs4 import BeautifulSoup

# 1. 發送 GET 請求
url = "https://mumu416.pixnet.net/blog/post/161886901?utm_source=chatgpt.com"
headers = {
    "User-Agent": "Mozilla/5.0"
}
response = requests.get(url, headers=headers)
response.encoding = 'utf-8'

# 2. 解析 HTML
soup = BeautifulSoup(response.text, 'html.parser')

# 3. 擷取標題
title_tag = soup.find('h2', class_='article-title')
title = title_tag.get_text(strip=True) if title_tag else '找不到標題'

for h2 in soup.find_all('h2'):
    a_tag = h2.find('a')
    if a_tag and a_tag.has_attr('href'):
        print("標題：", a_tag.get_text(strip=True))
        print("連結：", a_tag['href'])

# 4. 擷取內容
content_div = soup.find('div', class_='article-content-inner')
if content_div:
    paragraphs = content_div.find_all('p')
    content = '\n'.join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))
else:
    content = '找不到內容'

# 5. 顯示結果
print("\n內容：\n", content)

import requests
from bs4 import BeautifulSoup

# 1. 發送 GET 請求
url = "https://sujungyuo970926.pixnet.net/blog/post/345928714?utm_source=chatgpt.com"
headers = {
    "User-Agent": "Mozilla/5.0"
}
response = requests.get(url, headers=headers)
response.encoding = 'utf-8'

# 2. 解析 HTML
soup = BeautifulSoup(response.text, 'html.parser')

# 3. 擷取標題
title_tag = soup.find('h2', class_='article-title')
title = title_tag.get_text(strip=True) if title_tag else '找不到標題'

for h2 in soup.find_all('h2'):
    a_tag = h2.find('a')
    if a_tag and a_tag.has_attr('href'):
        print("標題：", a_tag.get_text(strip=True))
        print("連結：", a_tag['href'])

# 4. 擷取內容
content_div = soup.find('div', class_='article-content-inner')
if content_div:
    paragraphs = content_div.find_all('p')
    content = '\n'.join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))
else:
    content = '找不到內容'

# 5. 顯示結果
print("\n內容：\n", content)

import requests
from bs4 import BeautifulSoup

# 1. 發送 GET 請求
url = "https://helensdiary.pixnet.net/blog/post/366606154?utm_source=PIXNET&utm_medium=ppage"
headers = {
    "User-Agent": "Mozilla/5.0"
}
response = requests.get(url, headers=headers)
response.encoding = 'utf-8'

# 2. 解析 HTML
soup = BeautifulSoup(response.text, 'html.parser')

# 3. 擷取標題
title_tag = soup.find('h2', class_='article-title')
title = title_tag.get_text(strip=True) if title_tag else '找不到標題'

for h2 in soup.find_all('h2'):
    a_tag = h2.find('a')
    if a_tag and a_tag.has_attr('href'):
        print("標題：", a_tag.get_text(strip=True))
        print("連結：", a_tag['href'])

# 4. 擷取內容
content_div = soup.find('div', class_='article-content-inner')
if content_div:
    paragraphs = content_div.find_all('p')
    content = '\n'.join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))
else:
    content = '找不到內容'

# 5. 顯示結果
print("\n內容：\n", content)

import requests
from bs4 import BeautifulSoup


def main():
    url = "https://markandhazyl.com/camping-in-miaoli/"
    # 有些防爬蟲網站會檢查請求標頭 (request headers) 的user-agent是否有值，以辨識是一般使用者還是爬蟲程式訪問，來決定是否拒絕請求
    # 建議加上user-agent，值可以複製Chrome > F12 > Network > Headers > Request Headers > user-agent，來偽裝成一般使用者
    # The headers variable should be a dictionary, not a set.
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36"
    }
    response = requests.get(url, headers=headers)
    if response.status_code != 200:
        print(f"請求失敗，status code: {response.status_code}")
        return
    # 顯示結果網頁內容
    # print(response.text)
    # 建立BeautifulSoup物件
    soup = BeautifulSoup(response.text, "html.parser")
    # 先使用Chrome觀察一篇文章範圍，並用select_one()測試
    article = soup.find('h3')
    # print(f"article:\n{article}")
    # 取出標題
    title = article.get_text(strip=True)
    # print(f"title: {title}")
    # 取出連結
    # 當標題為「本文已被刪除」則<a>為空值
    a = article.select_one(".title > a")
    # print(f"a: {a}")
    link = a["href"] if a else None
    # 取出日期
    time_tag = soup.find('time')
    date = time_tag.get_text(strip=True)
    print(f"{title}\t{link}\n{date}")


if __name__ == "__main__":
    print("===================================")
    main()
    print("===================================")

import requests
from bs4 import BeautifulSoup

url = 'https://markandhazyl.com/camping-in-miaoli/'
response = requests.get(url)
response.encoding = 'utf-8'
soup = BeautifulSoup(response.text, 'html.parser')

# 抓出所有 h3 + p 組合
h3_tags = soup.find_all('h3')
for h3 in h3_tags:
    h3_text = h3.get_text(strip=True)
    next_p = h3.find_next_sibling('p')
    if next_p:
        p_text = next_p.get_text(strip=True)
        print(f"📍 {h3_text}\n➡ {p_text}\n")

"""#第一篇文章(格式不同)"""

import requests
from bs4 import BeautifulSoup
import pandas as pd


def getPageContent():

    url = "https://markandhazyl.com/camping-in-miaoli/"
    headers = {
        "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36"}
    response = requests.get(url, headers=headers)
    if response.status_code != 200:
        print(f"請求失敗，status code: {response.status_code}")
        return
    # 建立BeautifulSoup物件
    soup = BeautifulSoup(response.text, "html.parser")
    articles = []

    level1 = soup.select_one("div.entry-content.single-content")
    level2_title = level1.select("a > strong")[:17]
    # print(level2_title)
    for level3 in level2_title:
      title = level3.text.strip()
      link = level3.parent.get("href")
      level3_block = level3.find_parent("h3").next_siblings
      content = ""
      for p in level3_block:
        if p.name == "h3":
          break
        content += p.text

      #content = level3_block.find_next_sibling('p').get_text(strip=True)
      articles.append([title,link,content])

    # 將整頁所有文章轉存放至DataFrame
    df = pd.DataFrame(articles, columns=["title", "link", "content"])
    return df

def dataInfo(df):
    # 列出DataFrame資訊，以查詢哪些欄位有空值
    df.info()
    print(df)


def main():
    df = getPageContent()
    dataInfo(df)
    df.to_csv("articles.csv",encoding="utf-8")


if __name__ == "__main__":

    main()

"""# 第二篇文章"""

import requests
from bs4 import BeautifulSoup
import pandas as pd

url = "https://qqkelly.com/kaohuing/"
headers = {
    "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36"
}
response = requests.get(url, headers=headers)
soup = BeautifulSoup(response.content, 'html.parser')
articles=[]

# 抓標題
title_div = soup.find('div', class_='post-header')
title_tag = title_div.find('h1')
title = title_tag.text.strip()

# 抓發文日期
date_tag = title_div.find('span', class_='post-date')
date = date_tag.text.strip()

# 抓內容
content_div = soup.find('div', class_='post-entry')
p_tags = content_div.find_all('p')
content = "\n".join(p.text.strip() for p in p_tags)  # 把所有段落合併成一段文字，換行分開


articles.append([title,date,content])

df = pd.DataFrame(articles, columns=["title", "date", "content"])

# 放進DataFrame
df.info()
print(df)

df.to_csv("articles2.csv",encoding="utf-8")

"""# 第三篇文章"""

import requests
from bs4 import BeautifulSoup
import pandas as pd

url = "https://fumtravel.pixnet.net/blog/post/181212259?utm_source=PIXNET&utm_medium=ppage"
headers = {
    "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36"
}
response = requests.get(url, headers=headers)
soup = BeautifulSoup(response.content, 'html.parser')
articles=[]

# 抓標題
title_div = soup.find('div', class_='article')
title_tag = title_div.find('h2')
title = title_tag.text.strip()
print(title)

#發文日期
publish_li = soup.find('li', class_='publish')
if publish_li:
    text = publish_li.get_text(separator=' ', strip=True)
    print(text)
else:
    print("找不到 publish 的 li")



# 抓內容
content_div = soup.find('div', class_='article-content-inner')
p_tags = content_div.find_all('p')
content = "\n".join(p.text.strip() for p in p_tags)



articles.append([title,text,content])

df = pd.DataFrame(articles, columns=["title", "date", "content"])

# 放進DataFrame
df.info()
print(df)

df.to_csv("articles3.csv",encoding="utf-8")

"""# 第四篇文章(少了日期)"""

import requests
from bs4 import BeautifulSoup
import pandas as pd

url = "https://dpmm2021.pixnet.net/blog/post/67557052?utm_source=PIXNET&utm_medium=ppage"
headers = {
    "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36"
}
response = requests.get(url, headers=headers)
soup = BeautifulSoup(response.content, 'html.parser')
articles = []

# 抓標題
title_div = soup.find('div', class_='article')
title_tag = title_div.find('h2')
title = title_tag.text.strip()
print(title)

# 發文日期
publish_li = soup.find('li', class_='publish')
if publish_li:
    text = publish_li.get_text(separator=' ', strip=True)
    print(text)
else:
    print("找不到 publish 的 li")

content = ""
h4_tags = soup.select('h4')
# 逐一抓取h4標籤下的span標籤，再抓裡面的strong
for h4 in h4_tags[:96]:
    span_tag = h4.find('span')  # 找到h4下的span
    # Change is here. Instead of searching for a parent, iterate through siblings directly
    level3_block = h4.next_siblings
    content = ""
    for p in level3_block:
        if p.name == "h3":
            break
        if p.name == 'p': # Only consider <p> tags for content
            content += p.text.strip()
    if span_tag:
        strong_tag = span_tag.find('strong').text.strip()  # 找到span下的strong
        #print(strong_tag.text.strip())  # 輸出strong的內容

    articles.append([strong_tag, content])  # Append content, not p_tags


df = pd.DataFrame(articles, columns=["title", "content"])

# 放進DataFrame
df.info()
print(df)

df.to_csv("articles4.csv", encoding="utf-8")

"""# 第五篇文章"""

import requests
from bs4 import BeautifulSoup
import pandas as pd

url = "https://sujungyuo970926.pixnet.net/blog/post/347228242?utm_source=PIXNET&utm_medium=ppage"
headers = {
    "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36"
}
response = requests.get(url, headers=headers)
soup = BeautifulSoup(response.content, 'html.parser')
articles = []

# 抓標題
title_div = soup.find('div', class_='article')
title_tag = title_div.find('h2')
title = title_tag.text.strip()
print(title)

# 發文日期
publish_li = soup.find('li', class_='publish')
if publish_li:
    text = publish_li.get_text(separator=' ', strip=True)
    print(text)
else:
    print("找不到 publish 的 li")

content = ""
content_div = soup.find('div', class_='article-content-inner')
p_tags = content_div.find_all('p')
content = "\n".join(p.text.strip() for p in p_tags)

articles.append([title,text,content])


df = pd.DataFrame(articles, columns=["title", "date","content"])

# 放進DataFrame
df.info()
print(df)

df.to_csv("articles5.csv", encoding="utf-8")

"""# 第六篇文章"""

import requests
from bs4 import BeautifulSoup
import pandas as pd

url = "https://sunboy0722.pixnet.net/blog/post/358228299?utm_source=PIXNET&utm_medium=ppage"
headers = {
    "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36"
}
response = requests.get(url, headers=headers)
soup = BeautifulSoup(response.content, 'html.parser')
articles = []

# 抓標題
title_div = soup.find('div', class_='article')
title_tag = title_div.find('h2')
title = title_tag.text.strip()
print(title)

# 發文日期
publish_li = soup.find('li', class_='publish')
if publish_li:
    text = publish_li.get_text(separator=' ', strip=True)
    print(text)
else:
    print("找不到 publish 的 li")

content = ""
content_div = soup.find('div', class_='article-content-inner')
p_tags = content_div.find_all('p')[:213]
content = "\n".join(p.text.strip() for p in p_tags)

articles.append([title,text,content])


df = pd.DataFrame(articles, columns=["title", "date","content"])

# 放進DataFrame
df.info()
print(df)

df.to_csv("articles6.csv", encoding="utf-8")

"""# 第七篇文章"""

import requests
from bs4 import BeautifulSoup
import pandas as pd

url = "https://abc0601.pixnet.net/blog/post/347175517?utm_source=PIXNET&utm_medium=ppage"
headers = {
    "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36"
}
response = requests.get(url, headers=headers)
soup = BeautifulSoup(response.content, 'html.parser')
articles = []

# 抓標題
title_div = soup.find('div', class_='article')
title_tag = title_div.find('h2')
title = title_tag.text.strip()
print(title)

# 發文日期
publish_li = soup.find('li', class_='publish')
if publish_li:
    text = publish_li.get_text(separator=' ', strip=True)
    print(text)
else:
    print("找不到 publish 的 li")

content = ""
content_div = soup.find('div', class_='article-content-inner')
p_tags = content_div.find_all('p')
content = "\n".join(p.text.strip() for p in p_tags)

articles.append([title,text,content])


df = pd.DataFrame(articles, columns=["title", "date","content"])

# 放進DataFrame
df.info()
print(df)

df.to_csv("articles7.csv", encoding="utf-8")

"""# 第八篇文章"""

import requests
from bs4 import BeautifulSoup
import pandas as pd

url = "https://eeooa0314.pixnet.net/blog/post/577385944?utm_source=PIXNET&utm_medium=ppage"
headers = {
    "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36"
}
response = requests.get(url, headers=headers)
soup = BeautifulSoup(response.content, 'html.parser')
articles = []

# 抓標題
title_div = soup.find('div', class_='article')
title_tag = title_div.find('h2')
title = title_tag.text.strip()
print(title)

# 發文日期
publish_li = soup.find('li', class_='publish')
if publish_li:
    text = publish_li.get_text(separator=' ', strip=True)
    print(text)
else:
    print("找不到 publish 的 li")

content = ""
content_div = soup.find('div', class_='article-content-inner')
p_tags = content_div.find_all('p')
content = "\n".join(p.text.strip() for p in p_tags)

articles.append([title,text,content])


df = pd.DataFrame(articles, columns=["title", "date","content"])

# 放進DataFrame
df.info()
print(df)

df.to_csv("articles8.csv", encoding="utf-8")

"""# 第九篇文章"""

import requests
from bs4 import BeautifulSoup
import pandas as pd

url = "https://sunboy0722.pixnet.net/blog/post/358031400?utm_source=PIXNET&utm_medium=ppage"
headers = {
    "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36"
}
response = requests.get(url, headers=headers)
soup = BeautifulSoup(response.content, 'html.parser')
articles = []

# 抓標題
title_div = soup.find('div', class_='article')
title_tag = title_div.find('h2')
title = title_tag.text.strip()
print(title)

# 發文日期
publish_li = soup.find('li', class_='publish')
if publish_li:
    text = publish_li.get_text(separator=' ', strip=True)
    print(text)
else:
    print("找不到 publish 的 li")

content = ""
content_div = soup.find('div', class_='article-content-inner')
p_tags = content_div.find_all('p')
content = "\n".join(p.text.strip() for p in p_tags)

articles.append([title,text,content])


df = pd.DataFrame(articles, columns=["title", "date","content"])

# 放進DataFrame
df.info()
print(df)

df.to_csv("articles9.csv", encoding="utf-8")

"""# 第十篇文章"""

import requests
from bs4 import BeautifulSoup
import pandas as pd

url = "https://abcdef12332114.pixnet.net/blog/post/222897223?utm_source=PIXNET&utm_medium=ppage"
headers = {
    "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36"
}
response = requests.get(url, headers=headers)
soup = BeautifulSoup(response.content, 'html.parser')
articles = []

# 抓標題
title_div = soup.find('div', class_='article')
title_tag = title_div.find('h2')
title = title_tag.text.strip()
print(title)

# 發文日期
publish_li = soup.find('li', class_='publish')
if publish_li:
    text = publish_li.get_text(separator=' ', strip=True)
    print(text)
else:
    print("找不到 publish 的 li")

content = ""
content_div = soup.find('div', class_='article-content-inner')
p_tags = content_div.find_all('p')
content = "\n".join(p.text.strip() for p in p_tags)

articles.append([title,text,content])


df = pd.DataFrame(articles, columns=["title", "date","content"])

# 放進DataFrame
df.info()
print(df)

df.to_csv("articles10.csv", encoding="utf-8")

"""# 第十一篇文章"""

import requests
from bs4 import BeautifulSoup
import pandas as pd

url = "https://sujungyuo970926.pixnet.net/blog/post/346623334?utm_source=PIXNET&utm_medium=ppage"
headers = {
    "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36"
}
response = requests.get(url, headers=headers)
soup = BeautifulSoup(response.content, 'html.parser')
articles = []

# 抓標題
title_div = soup.find('div', class_='article')
title_tag = title_div.find('h2')
title = title_tag.text.strip()
print(title)

# 發文日期
publish_li = soup.find('li', class_='publish')
if publish_li:
    text = publish_li.get_text(separator=' ', strip=True)
    print(text)
else:
    print("找不到 publish 的 li")

content = ""
content_div = soup.find('div', class_='article-content-inner')
p_tags = content_div.find_all('p')
content = "\n".join(p.text.strip() for p in p_tags)

articles.append([title,text,content])


df = pd.DataFrame(articles, columns=["title", "date","content"])

# 放進DataFrame
df.info()
print(df)

df.to_csv("articles11.csv", encoding="utf-8")

"""# 第十二篇文章"""

import requests
from bs4 import BeautifulSoup
import pandas as pd

url = "https://paicj.pixnet.net/blog/post/407084009?utm_source=PIXNET&utm_medium=ppage"
headers = {
    "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36"
}
response = requests.get(url, headers=headers)
soup = BeautifulSoup(response.content, 'html.parser')
articles = []

# 抓標題
title_div = soup.find('div', class_='article')
title_tag = title_div.find('h2')
title = title_tag.text.strip()
print(title)

# 發文日期
publish_li = soup.find('li', class_='publish')
if publish_li:
    text = publish_li.get_text(separator=' ', strip=True)
    print(text)
else:
    print("找不到 publish 的 li")

content = ""
content_div = soup.find('div', class_='article-content-inner')
p_tags = content_div.find_all('p')
content = "\n".join(p.text.strip() for p in p_tags)

articles.append([title,text,content])


df = pd.DataFrame(articles, columns=["title", "date","content"])

# 放進DataFrame
df.info()
print(df)

df.to_csv("articles12.csv", encoding="utf-8")

"""# 第十三篇文章"""

import requests
from bs4 import BeautifulSoup
import pandas as pd

url = "https://evonne1205.pixnet.net/blog/post/165797605?utm_source=PIXNET&utm_medium=ppage"
headers = {
    "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36"
}
response = requests.get(url, headers=headers)
soup = BeautifulSoup(response.content, 'html.parser')
articles = []

# 抓標題
title_div = soup.find('div', class_='article')
title_tag = title_div.find('h2')
title = title_tag.text.strip()
print(title)

# 發文日期
publish_li = soup.find('li', class_='publish')
if publish_li:
    text = publish_li.get_text(separator=' ', strip=True)
    print(text)
else:
    print("找不到 publish 的 li")

content = ""
content_div = soup.find('div', class_='article-content-inner')
p_tags = content_div.find_all('p')
content = "\n".join(p.text.strip() for p in p_tags)

articles.append([title,text,content])


df = pd.DataFrame(articles, columns=["title", "date","content"])

# 放進DataFrame
df.info()
print(df)

df.to_csv("articles13.csv", encoding="utf-8")

"""# 第十四篇文章"""

import requests
from bs4 import BeautifulSoup
import pandas as pd

url = "https://cycle1003.pixnet.net/blog/post/163937233?utm_source=PIXNET&utm_medium=ppage"
headers = {
    "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36"
}
response = requests.get(url, headers=headers)
soup = BeautifulSoup(response.content, 'html.parser')
articles = []

# 抓標題
title_div = soup.find('div', class_='article')
title_tag = title_div.find('h2')
title = title_tag.text.strip()
print(title)

# 發文日期
publish_li = soup.find('li', class_='publish')
if publish_li:
    text = publish_li.get_text(separator=' ', strip=True)
    print(text)
else:
    print("找不到 publish 的 li")

content = ""
content_div = soup.find('div', class_='article-content-inner')
p_tags = content_div.find_all('p')
content = "\n".join(p.text.strip() for p in p_tags)

articles.append([title,text,content])


df = pd.DataFrame(articles, columns=["title", "date","content"])

# 放進DataFrame
df.info()
print(df)

df.to_csv("articles14.csv", encoding="utf-8")

"""# 第十五篇文章"""

import requests
from bs4 import BeautifulSoup
import pandas as pd

url = "https://helensdiary.pixnet.net/blog/post/366606154?utm_source=PIXNET&utm_medium=ppage"
headers = {
    "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36"
}
response = requests.get(url, headers=headers)
soup = BeautifulSoup(response.content, 'html.parser')
articles = []

# 抓標題
title_div = soup.find('div', class_='article')
title_tag = title_div.find('h2')
title = title_tag.text.strip()
print(title)

# 發文日期
publish_li = soup.find('li', class_='publish')
if publish_li:
    text = publish_li.get_text(separator=' ', strip=True)
    print(text)
else:
    print("找不到 publish 的 li")

content = ""
content_div = soup.find('div', class_='article-content-inner')
p_tags = content_div.find_all('p')
content = "\n".join(p.text.strip() for p in p_tags)

articles.append([title,text,content])


df = pd.DataFrame(articles, columns=["title", "date","content"])

# 放進DataFrame
df.info()
print(df)

df.to_csv("articles15.csv", encoding="utf-8")

"""# 第十六篇文章"""

import requests
from bs4 import BeautifulSoup
import pandas as pd

url = "https://jack74327.pixnet.net/blog/post/71285884?utm_source=PIXNET&utm_medium=ppage"
headers = {
    "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36"
}
response = requests.get(url, headers=headers)
soup = BeautifulSoup(response.content, 'html.parser')
articles = []

# 抓標題
title_div = soup.find('div', class_='article')
title_tag = title_div.find('h2')
title = title_tag.text.strip()
print(title)

# 發文日期
publish_li = soup.find('li', class_='publish')
if publish_li:
    text = publish_li.get_text(separator=' ', strip=True)
    print(text)
else:
    print("找不到 publish 的 li")

content = ""
content_div = soup.find('div', class_='article-content-inner')
p_tags = content_div.find_all('p')
content = "\n".join(p.text.strip() for p in p_tags)

articles.append([title,text,content])


df = pd.DataFrame(articles, columns=["title", "date","content"])

# 放進DataFrame
df.info()
print(df)

df.to_csv("articles16.csv", encoding="utf-8")

"""# 第十七篇文章"""

# 匯入套件
import requests
from bs4 import BeautifulSoup
import pandas as pd

# 定義爬蟲函式
def scrape_pixnet_article(url: str, save_to_csv: bool = False, filename: str = "articles.csv") -> pd.DataFrame:
    headers = {
        "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36"
    }
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.content, 'html.parser')
    articles = []

    # 抓標題
    title_div = soup.find('div', class_='article')
    title_tag = title_div.find('h2') if title_div else None
    title = title_tag.text.strip() if title_tag else "找不到標題"

    # 發文日期
    publish_li = soup.find('li', class_='publish')
    text = publish_li.get_text(separator=' ', strip=True) if publish_li else "找不到發文日期"

    # 抓內容
    content_div = soup.find('div', class_='article-content-inner')
    if content_div:
        p_tags = content_div.find_all('p')
        content = "\n".join(p.text.strip() for p in p_tags)
    else:
        content = "找不到內容區塊"

    articles.append([title, text, content])
    df = pd.DataFrame(articles, columns=["title", "date", "content"])

    if save_to_csv:
        df.to_csv(filename, encoding="utf-8", index=False)

    return df

# 主程式入口
if __name__ == "__main__":
    # 輸入你想爬的 PIXNET 網址
    url = "https://icela.pixnet.net/blog/post/49374036?utm_source=PIXNET&utm_medium=ppage"

    # 呼叫函式，爬取文章並存成 CSV
    df = scrape_pixnet_article(url, save_to_csv=True, filename="articles17.csv")

    # 印出 DataFrame 結果
    print(df)

"""# 第十八篇文章"""

# 匯入套件
import requests
from bs4 import BeautifulSoup
import pandas as pd

# 定義爬蟲函式
def scrape_pixnet_article(url: str, save_to_csv: bool = False, filename: str = "articles.csv") -> pd.DataFrame:
    headers = {
        "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36"
    }
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.content, 'html.parser')
    articles = []

    # 抓標題
    title_div = soup.find('div', class_='article')
    title_tag = title_div.find('h2') if title_div else None
    title = title_tag.text.strip() if title_tag else "找不到標題"

    # 發文日期
    publish_li = soup.find('li', class_='publish')
    text = publish_li.get_text(separator=' ', strip=True) if publish_li else "找不到發文日期"

    # 抓內容
    content_div = soup.find('div', class_='article-content-inner')
    if content_div:
        p_tags = content_div.find_all('p')
        content = "\n".join(p.text.strip() for p in p_tags)
    else:
        content = "找不到內容區塊"

    articles.append([title, text, content])
    df = pd.DataFrame(articles, columns=["title", "date", "content"])

    if save_to_csv:
        df.to_csv(filename, encoding="utf-8", index=False)

    return df

# 主程式入口
if __name__ == "__main__":
    # 輸入你想爬的 PIXNET 網址
    url = "https://sujungyuo970926.pixnet.net/blog/post/345928714?utm_source=PIXNET&utm_medium=ppage"
    # 呼叫函式，爬取文章並存成 CSV
    df = scrape_pixnet_article(url, save_to_csv=True, filename="articles18.csv")

    # 印出 DataFrame 結果
    print(df)

"""# 寫成函式"""

import requests
from bs4 import BeautifulSoup
import pandas as pd

# 定義爬蟲函式
def scrape_pixnet_article(url: str) -> pd.DataFrame:
    headers = {
        "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36"
    }
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.content, 'html.parser')

    # 抓標題
    title_div = soup.find('div', class_='article')
    title_tag = title_div.find('h2') if title_div else None
    title = title_tag.text.strip() if title_tag else "找不到標題"

    # 發文日期
    publish_li = soup.find('li', class_='publish')
    date = publish_li.get_text(separator=' ', strip=True) if publish_li else "找不到發文日期"

    # 抓內容
    content_div = soup.find('div', class_='article-content-inner')
    if content_div:
        p_tags = content_div.find_all('p')
        content = "\n".join(p.text.strip() for p in p_tags)
    else:
        content = "找不到內容區塊"

    return pd.DataFrame([[title, date, content]], columns=["title", "date", "content"])

# 主程式：多網址、多CSV檔案
if __name__ == "__main__":
    urls = [
        "https://evshhips.pixnet.net/blog/post/357477369?utm_source=PIXNET&utm_medium=ppage",
        "https://chant198983.pixnet.net/blog/post/234053553?utm_source=PIXNET&utm_medium=ppage",
        "https://chant198983.pixnet.net/blog/post/234049989?utm_source=PIXNET&utm_medium=ppage",
        # 你可以繼續加入更多網址
    ]

    start_index = 19  # 從 articles18.csv 開始命名

    for i, url in enumerate(urls, start=start_index):
        try:
            df = scrape_pixnet_article(url)
            filename = f"articles{i}.csv"
            df.to_csv(filename, encoding="utf-8", index=False)
            print(f"✅ 儲存成功：{filename}")
        except Exception as e:
            print(f"❌ 錯誤：{url} -> {e}")

import requests
from bs4 import BeautifulSoup
import pandas as pd

# 定義爬蟲函式
def scrape_pixnet_article(url: str) -> pd.DataFrame:
    headers = {
        "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36"
    }
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.content, 'html.parser')

    # 抓標題
    title_div = soup.find('div', class_='article')
    title_tag = title_div.find('h2') if title_div else None
    title = title_tag.text.strip() if title_tag else "找不到標題"

    # 發文日期
    publish_li = soup.find('li', class_='publish')
    date = publish_li.get_text(separator=' ', strip=True) if publish_li else "找不到發文日期"

    # 抓內容
    content_div = soup.find('div', class_='article-content-inner')
    if content_div:
        p_tags = content_div.find_all('p')
        content = "\n".join(p.text.strip() for p in p_tags)
    else:
        content = "找不到內容區塊"

    return pd.DataFrame([[title, date, content]], columns=["title", "date", "content"])

# 主程式：多網址、多CSV檔案
if __name__ == "__main__":
    urls = [
        "https://kk961208.pixnet.net/blog/post/576743152?utm_source=PIXNET&utm_medium=ppage",
        "https://spirit0926.pixnet.net/blog/post/50852228?utm_source=PIXNET&utm_medium=ppage",
        "https://campingshark86.pixnet.net/blog/post/155059279?utm_source=PIXNET&utm_medium=ppage",
        "https://sujungyuo970926.pixnet.net/blog/post/345498550?utm_source=PIXNET&utm_medium=ppage"
        # 你可以繼續加入更多網址
    ]

    start_index = 22  # 從 articles18.csv 開始命名

    for i, url in enumerate(urls, start=start_index):
        try:
            df = scrape_pixnet_article(url)
            filename = f"articles{i}.csv"
            df.to_csv(filename, encoding="utf-8", index=False)
            print(f"✅ 儲存成功：{filename}")
        except Exception as e:
            print(f"❌ 錯誤：{url} -> {e}")

import requests
from bs4 import BeautifulSoup
import pandas as pd

# 定義爬蟲函式
def scrape_pixnet_article(url: str) -> pd.DataFrame:
    headers = {
        "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36"
    }
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.content, 'html.parser')

    # 抓標題
    title_div = soup.find('div', class_='article')
    title_tag = title_div.find('h2') if title_div else None
    title = title_tag.text.strip() if title_tag else "找不到標題"

    # 發文日期
    publish_li = soup.find('li', class_='publish')
    date = publish_li.get_text(separator=' ', strip=True) if publish_li else "找不到發文日期"

    # 抓內容
    content_div = soup.find('div', class_='article-content-inner')
    if content_div:
        p_tags = content_div.find_all('p')
        content = "\n".join(p.text.strip() for p in p_tags)
    else:
        content = "找不到內容區塊"

    return pd.DataFrame([[title, date, content]], columns=["title", "date", "content"])

# 主程式：多網址、多CSV檔案
if __name__ == "__main__":
    urls = [
     "https://pimama.pixnet.net/blog/post/152847460?utm_source=PIXNET&utm_medium=ppage",
     "https://wtw332589.pixnet.net/blog/post/345111304?utm_source=PIXNET&utm_medium=ppage",
     "https://fan202189.pixnet.net/blog/post/151067122?utm_source=PIXNET&utm_medium=ppage",
     "https://umechen.pixnet.net/blog/post/34019597?utm_source=PIXNET&utm_medium=ppage",
     "https://wtw332589.pixnet.net/blog/post/344783791?utm_source=PIXNET&utm_medium=ppage",
     "https://monkeybowbi.pixnet.net/blog/post/576113508?utm_source=PIXNET&utm_medium=ppage",
     "https://vivi0010.pixnet.net/blog/post/233723769?utm_source=PIXNET&utm_medium=ppage",
     "https://hara5415.pixnet.net/blog/post/355623421?utm_source=PIXNET&utm_medium=ppage",
     "https://pigpig0412.pixnet.net/blog/post/233020036?utm_source=PIXNET&utm_medium=ppage",
     "https://q3621533.pixnet.net/blog/post/569110372?utm_source=PIXNET&utm_medium=ppage",
     "https://sammi0224.pixnet.net/blog/post/121968776?utm_source=PIXNET&utm_medium=ppage",
     "https://win960927.pixnet.net/blog/post/576034040?utm_source=PIXNET&utm_medium=ppage",
     "https://f405510017.pixnet.net/blog/post/341933520?utm_source=PIXNET&utm_medium=ppage",
     "https://sammima.pixnet.net/blog/post/121713461?utm_source=PIXNET&utm_medium=ppage",
     "https://hochusay.pixnet.net/blog/post/102176588?utm_source=PIXNET&utm_medium=ppage",
     "https://tom20030208.pixnet.net/blog/post/339989645?utm_source=PIXNET&utm_medium=ppage",
     "https://ctinas604.pixnet.net/blog/post/340289715?utm_source=PIXNET&utm_medium=ppage",
     "https://unatsai525.pixnet.net/blog/post/571181516?utm_source=PIXNET&utm_medium=ppage",
     "https://anity0404.pixnet.net/blog/post/232514797?utm_source=PIXNET&utm_medium=ppage",
     "https://hara5415.pixnet.net/blog/post/354633634?utm_source=PIXNET&utm_medium=ppage",
     "https://joycerb.pixnet.net/blog/post/336962477?utm_source=PIXNET&utm_medium=ppage",
     "https://pinke0324.pixnet.net/blog/post/565055590?utm_source=PIXNET&utm_medium=ppage",
     "https://r510517.pixnet.net/blog/post/335249041?utm_source=PIXNET&utm_medium=ppage",
     "https://sic27ap.pixnet.net/blog/post/50067124?utm_source=PIXNET&utm_medium=ppage",
     "https://pbecky77.pixnet.net/blog/post/355242367?utm_source=PIXNET&utm_medium=ppage",
     "https://liankaogo.pixnet.net/blog/post/55329370?utm_source=PIXNET&utm_medium=ppage",
     "https://q3621533.pixnet.net/blog/post/565065358?utm_source=PIXNET&utm_medium=ppage"

      # 你可以繼續加入更多網址
    ]

    start_index = 26  # 從 articles18.csv 開始命名

    for i, url in enumerate(urls, start=start_index):
        try:
            df = scrape_pixnet_article(url)
            filename = f"articles{i}.csv"
            df.to_csv(filename, encoding="utf-8", index=False)
            print(f"✅ 儲存成功：{filename}")
        except Exception as e:
            print(f"❌ 錯誤：{url} -> {e}")

import requests
from bs4 import BeautifulSoup
import pandas as pd

# 定義爬蟲函式
def scrape_pixnet_article(url: str) -> pd.DataFrame:
    headers = {
        "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36"
    }
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.content, 'html.parser')

    # 抓標題
    title_div = soup.find('div', class_='article')
    title_tag = title_div.find('h2') if title_div else None
    title = title_tag.text.strip() if title_tag else "找不到標題"

    # 發文日期
    publish_li = soup.find('li', class_='publish')
    date = publish_li.get_text(separator=' ', strip=True) if publish_li else "找不到發文日期"

    # 抓內容
    content_div = soup.find('div', class_='article-content-inner')
    if content_div:
        p_tags = content_div.find_all('p')
        content = "\n".join(p.text.strip() for p in p_tags)
    else:
        content = "找不到內容區塊"

    return pd.DataFrame([[title, date, content]], columns=["title", "date", "content"])

# 主程式：多網址、多CSV檔案
if __name__ == "__main__":
    urls = [
     "https://jarvi2s.pixnet.net/blog/post/61074877?utm_source=PIXNET&utm_medium=ppage",
     "https://pei95416.pixnet.net/blog/post/362602541?utm_source=PIXNET&utm_medium=ppage",
     "https://wakabayashi.pixnet.net/blog/post/63922354?utm_source=PIXNET&utm_medium=ppage",
     "https://jush.pixnet.net/blog/post/225975101?utm_source=PIXNET&utm_medium=ppage",
     "https://ywayway.pixnet.net/blog/post/121539864?utm_source=PIXNET&utm_medium=ppage",
     "https://openchiang1113.pixnet.net/blog/post/121400824?utm_source=PIXNET&utm_medium=ppage",
     "https://openchiang1113.pixnet.net/blog/post/121339288?utm_source=PIXNET&utm_medium=ppage",
     "https://kikimilk2.pixnet.net/blog/post/336223181?utm_source=PIXNET&utm_medium=ppage",
     "https://kikimilk2.pixnet.net/blog/post/336605697?utm_source=PIXNET&utm_medium=ppage",
     "https://chiafei123.pixnet.net/blog/post/362798901?utm_source=PIXNET&utm_medium=ppage",
     "https://ctinas604.pixnet.net/blog/post/339463815?utm_source=PIXNET&utm_medium=ppage",
     "https://mikaaaaa0907.pixnet.net/blog/post/226150231?utm_source=PIXNET&utm_medium=ppage",
     "https://hara5415.pixnet.net/blog/post/354059587?utm_source=PIXNET&utm_medium=ppage",
     "https://meiworld.pixnet.net/blog/post/53612593?utm_source=PIXNET&utm_medium=ppage",
     "https://pbecky77.pixnet.net/blog/post/353872414?utm_source=PIXNET&utm_medium=ppage",
     "https://jnr283.pixnet.net/blog/post/225325315?utm_source=PIXNET&utm_medium=ppage"

      # 你可以繼續加入更多網址
    ]

    start_index = 53  # 從 articles18.csv 開始命名

    for i, url in enumerate(urls, start=start_index):
        try:
            df = scrape_pixnet_article(url)
            filename = f"articles{i}.csv"
            df.to_csv(filename, encoding="utf-8", index=False)
            print(f"✅ 儲存成功：{filename}")
        except Exception as e:
            print(f"❌ 錯誤：{url} -> {e}")

import requests
from bs4 import BeautifulSoup
import pandas as pd

# 定義爬蟲函式
def scrape_pixnet_article(url: str) -> pd.DataFrame:
    headers = {
        "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36"
    }
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.content, 'html.parser')

    # 抓標題
    title_div = soup.find('div', class_='article')
    title_tag = title_div.find('h2') if title_div else None
    title = title_tag.text.strip() if title_tag else "找不到標題"

    # 發文日期
    publish_li = soup.find('li', class_='publish')
    date = publish_li.get_text(separator=' ', strip=True) if publish_li else "找不到發文日期"

    # 抓內容
    content_div = soup.find('div', class_='article-content-inner')
    if content_div:
        p_tags = content_div.find_all('p')
        content = "\n".join(p.text.strip() for p in p_tags)
    else:
        content = "找不到內容區塊"

    return pd.DataFrame([[title, date, content]], columns=["title", "date", "content"])

# 主程式：多網址、多CSV檔案
if __name__ == "__main__":
    urls = [
     "https://veneaus.pixnet.net/blog/post/329941240?utm_source=PIXNET&utm_medium=ppage",
     "https://gin762001.pixnet.net/blog/post/42156274?utm_source=PIXNET&utm_medium=ppage",
     "https://gin762001.pixnet.net/blog/post/54491344?utm_source=PIXNET&utm_medium=ppage",
     "https://chunella726.pixnet.net/blog/post/353589366?utm_source=PIXNET&utm_medium=ppage",
     "https://a9x22t3eh.pixnet.net/blog/post/354367369?utm_source=PIXNET&utm_medium=ppage",
     "https://kikimilk2.pixnet.net/blog/post/332886601?utm_source=PIXNET&utm_medium=ppage",
     "https://smilefishfish.pixnet.net/blog/post/404953714?utm_source=PIXNET&utm_medium=ppage",
     "https://wowitspeggy.pixnet.net/blog/post/556841071?utm_source=PIXNET&utm_medium=ppage",
     "https://tadli.pixnet.net/blog/post/229930776?utm_source=PIXNET&utm_medium=ppage",
     "https://tadli.pixnet.net/blog/post/229706105?utm_source=PIXNET&utm_medium=ppage",
     "https://jmy7296.pixnet.net/blog/post/120799565?utm_source=PIXNET&utm_medium=ppage",
     "https://ni70043.pixnet.net/blog/post/351701242?utm_source=PIXNET&utm_medium=ppage",
     "https://weibaby0109.pixnet.net/blog/post/68693681?utm_source=PIXNET&utm_medium=ppage",
     "https://sunny7028.pixnet.net/blog/post/359403376?utm_source=PIXNET&utm_medium=ppage",
     "https://even615.pixnet.net/blog/post/120011586?utm_source=PIXNET&utm_medium=ppage",
     "https://e583i.pixnet.net/blog/post/67773750?utm_source=PIXNET&utm_medium=ppage",
     "https://e583i.pixnet.net/blog/post/67434423?utm_source=PIXNET&utm_medium=ppage",
     "https://bbpeng2.pixnet.net/blog/post/228740768?utm_source=PIXNET&utm_medium=ppage",
     "https://yingjun1109.pixnet.net/blog/post/35018896?utm_source=PIXNET&utm_medium=ppage",
     "https://lilstep.pixnet.net/blog/post/173117754?utm_source=PIXNET&utm_medium=ppage"


      # 你可以繼續加入更多網址
    ]

    start_index = 69  # 從 articles18.csv 開始命名

    for i, url in enumerate(urls, start=start_index):
        try:
            df = scrape_pixnet_article(url)
            filename = f"articles{i}.csv"
            df.to_csv(filename, encoding="utf-8", index=False)
            print(f"✅ 儲存成功：{filename}")
        except Exception as e:
            print(f"❌ 錯誤：{url} -> {e}")

import requests
from bs4 import BeautifulSoup
import pandas as pd

# 定義爬蟲函式
def scrape_pixnet_article(url: str) -> pd.DataFrame:
    headers = {
        "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36"
    }
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.content, 'html.parser')

    # 抓標題
    title_div = soup.find('div', class_='article')
    title_tag = title_div.find('h2') if title_div else None
    title = title_tag.text.strip() if title_tag else "找不到標題"

    # 發文日期
    publish_li = soup.find('li', class_='publish')
    date = publish_li.get_text(separator=' ', strip=True) if publish_li else "找不到發文日期"

    # 抓內容
    content_div = soup.find('div', class_='article-content-inner')
    if content_div:
        p_tags = content_div.find_all('p')
        content = "\n".join(p.text.strip() for p in p_tags)
    else:
        content = "找不到內容區塊"

    return pd.DataFrame([[title, date, content]], columns=["title", "date", "content"])

# 主程式：多網址、多CSV檔案
if __name__ == "__main__":
    urls = [
     "https://gn10202000.pixnet.net/blog/post/466533818?utm_source=PIXNET&utm_medium=ppage",
     "https://realobasan.pixnet.net/blog/post/264050196?utm_source=PIXNET&utm_medium=ppage",
     "https://bbpeng2.pixnet.net/blog/post/216870913?utm_source=PIXNET&utm_medium=ppage",
     "https://bbpeng2.pixnet.net/blog/post/222291115?utm_source=PIXNET&utm_medium=ppage",
     "https://caramelmm.pixnet.net/blog/post/301192867?utm_source=PIXNET&utm_medium=ppage#google_vignette",
     "https://hohopig.pixnet.net/blog/post/450205724?utm_source=PIXNET&utm_medium=ppage",
     "https://hohopig.pixnet.net/blog/post/447248663?utm_source=PIXNET&utm_medium=ppage",
     "https://bbpeng2.pixnet.net/blog/post/221060027?utm_source=PIXNET&utm_medium=ppage",
     "https://yumi3255.pixnet.net/blog/post/288964081?utm_source=PIXNET&utm_medium=ppage",
     "https://sammi0224.pixnet.net/blog/post/112621492?utm_source=PIXNET&utm_medium=ppage"


      # 你可以繼續加入更多網址
    ]

    start_index = 89  # 從 articles18.csv 開始命名

    for i, url in enumerate(urls, start=start_index):
        try:
            df = scrape_pixnet_article(url)
            filename = f"articles{i}.csv"
            df.to_csv(filename, encoding="utf-8", index=False)
            print(f"✅ 儲存成功：{filename}")
        except Exception as e:
            print(f"❌ 錯誤：{url} -> {e}")

import requests
from bs4 import BeautifulSoup
import pandas as pd

# 定義爬蟲函式
def scrape_pixnet_article(url: str) -> pd.DataFrame:
    headers = {
        "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36"
    }
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.content, 'html.parser')

    # 抓標題
    title_div = soup.find('div', class_='article')
    title_tag = title_div.find('h2') if title_div else None
    title = title_tag.text.strip() if title_tag else "找不到標題"

    # 發文日期
    publish_li = soup.find('li', class_='publish')
    date = publish_li.get_text(separator=' ', strip=True) if publish_li else "找不到發文日期"

    # 抓內容
    content_div = soup.find('div', class_='article-content-inner')
    if content_div:
        p_tags = content_div.find_all('p')
        content = "\n".join(p.text.strip() for p in p_tags)
    else:
        content = "找不到內容區塊"

    return pd.DataFrame([[title, date, content]], columns=["title", "date", "content"])

# 主程式：多網址、多CSV檔案
if __name__ == "__main__":
    urls = [
     "https://yourich.pixnet.net/blog/post/358307730?utm_source=PIXNET&utm_medium=ppage",
     "https://unatsai525.pixnet.net/blog/post/572311028",
     "https://paicj.pixnet.net/blog/post/407211841?utm_source=PIXNET&utm_medium=ppage"




      # 你可以繼續加入更多網址
    ]

    start_index = 99  # 從 articles18.csv 開始命名

    for i, url in enumerate(urls, start=start_index):
        try:
            df = scrape_pixnet_article(url)
            filename = f"articles{i}.csv"
            df.to_csv(filename, encoding="utf-8", index=False)
            print(f"✅ 儲存成功：{filename}")
        except Exception as e:
            print(f"❌ 錯誤：{url} -> {e}")

import requests
from bs4 import BeautifulSoup
import pandas as pd

# 定義爬蟲函式
def scrape_pixnet_article(url: str) -> pd.DataFrame:
    headers = {
        "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36"
    }
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.content, 'html.parser')

    # 抓標題
    title_div = soup.find('div', class_='article')
    title_tag = title_div.find('h2') if title_div else None
    title = title_tag.text.strip() if title_tag else "找不到標題"

    # 發文日期
    publish_li = soup.find('li', class_='publish')
    date = publish_li.get_text(separator=' ', strip=True) if publish_li else "找不到發文日期"

    # 抓內容
    content_div = soup.find('div', class_='article-content-inner')
    if content_div:
        p_tags = content_div.find_all('p')
        content = "\n".join(p.text.strip() for p in p_tags)
    else:
        content = "找不到內容區塊"

    return pd.DataFrame([[title, date, content]], columns=["title", "date", "content"])

# 主程式：多網址、多CSV檔案
if __name__ == "__main__":
    urls = [
    "https://sunboy0722.pixnet.net/blog/post/358293774?utm_source=PIXNET&utm_medium=ppage",
    "https://paicj.pixnet.net/blog/post/407201177?utm_source=PIXNET&utm_medium=ppage",
    "https://sunboy0722.pixnet.net/blog/post/357915222?utm_source=PIXNET&utm_medium=ppage",
    "https://liankaogo.pixnet.net/blog/post/167720920?utm_source=PIXNET&utm_medium=ppage",
    "https://aaa22034381.pixnet.net/blog/post/165951676?utm_source=PIXNET&utm_medium=ppage",
    "https://aaa22034381.pixnet.net/blog/post/166209856?utm_source=PIXNET&utm_medium=ppage",
    "https://huakuanbin.pixnet.net/blog/post/366697744?utm_source=PIXNET&utm_medium=ppage",
    "https://lilyliketoeat594.pixnet.net/blog/post/162573973?utm_source=PIXNET&utm_medium=ppage",
    "https://huakuanbin.pixnet.net/blog/post/366617770?utm_source=PIXNET&utm_medium=ppage",
    "https://showwen1011.pixnet.net/blog/post/346099969?utm_source=PIXNET&utm_medium=ppage",
    "https://abby0318.pixnet.net/blog/post/576910492?utm_source=PIXNET&utm_medium=ppage",
    "https://evshhips.pixnet.net/blog/post/357307161?utm_source=PIXNET&utm_medium=ppage",
    "https://cndjourney.pixnet.net/blog/post/341828409?utm_source=PIXNET&utm_medium=ppage",
    "https://sunboy0722.pixnet.net/blog/post/357345927?utm_source=PIXNET&utm_medium=ppage",
    "https://tadli.pixnet.net/blog/post/234440256?utm_source=PIXNET&utm_medium=ppage",
    "https://sunboy0722.pixnet.net/blog/post/358004763?utm_source=PIXNET&utm_medium=ppage",
    "https://sunboy0722.pixnet.net/blog/post/357959940?utm_source=PIXNET&utm_medium=ppage",
    "https://paicj.pixnet.net/blog/post/407108917?utm_source=PIXNET&utm_medium=ppage",
    "https://white91.pixnet.net/blog/post/169470331?utm_source=PIXNET&utm_medium=ppage",
    "https://lilyliketoeat594.pixnet.net/blog/post/169269448?utm_source=PIXNET&utm_medium=ppage",
    "https://chant198983.pixnet.net/blog/post/234551052?utm_source=PIXNET&utm_medium=ppage",
    "https://ysm1121321.pixnet.net/blog/post/366855304?utm_source=PIXNET&utm_medium=ppage"



      # 你可以繼續加入更多網址
    ]

    start_index = 102  # 從 articles18.csv 開始命名

    for i, url in enumerate(urls, start=start_index):
        try:
            df = scrape_pixnet_article(url)
            filename = f"articles{i}.csv"
            df.to_csv(filename, encoding="utf-8", index=False)
            print(f"✅ 儲存成功：{filename}")
        except Exception as e:
            print(f"❌ 錯誤：{url} -> {e}")

import requests
from bs4 import BeautifulSoup
import pandas as pd

# 定義爬蟲函式
def scrape_pixnet_article(url: str) -> pd.DataFrame:
    headers = {
        "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36"
    }
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.content, 'html.parser')

    # 抓標題
    title_div = soup.find('div', class_='article')
    title_tag = title_div.find('h2') if title_div else None
    title = title_tag.text.strip() if title_tag else "找不到標題"

    # 發文日期
    publish_li = soup.find('li', class_='publish')
    date = publish_li.get_text(separator=' ', strip=True) if publish_li else "找不到發文日期"

    # 抓內容
    content_div = soup.find('div', class_='article-content-inner')
    if content_div:
        p_tags = content_div.find_all('p')
        content = "\n".join(p.text.strip() for p in p_tags)
    else:
        content = "找不到內容區塊"

    return pd.DataFrame([[title, date, content]], columns=["title", "date", "content"])

# 主程式：多網址、多CSV檔案
if __name__ == "__main__":
    urls = [
   "https://pimama.pixnet.net/blog/post/152852305?utm_source=PIXNET&utm_medium=ppage",
   "https://hsinchueric.pixnet.net/blog/post/149364976?utm_source=PIXNET&utm_medium=ppage",
   "https://lovebaby31.pixnet.net/blog/post/222522742?utm_source=PIXNET&utm_medium=ppage",
   "https://camptrip.pixnet.net/blog/post/132346114?utm_source=PIXNET&utm_medium=ppage",
   "https://hsinchueric.pixnet.net/blog/post/140530129?utm_source=PIXNET&utm_medium=ppage",
   "https://ctinas604.pixnet.net/blog/post/343974406?utm_source=PIXNET&utm_medium=ppage",
   "https://camellia8283.pixnet.net/blog/post/100594817?utm_source=PIXNET&utm_medium=ppage",
   "https://liankaogo.pixnet.net/blog/post/100982982?utm_source=PIXNET&utm_medium=ppage",
   "https://yunjie059.pixnet.net/blog/post/104826901?utm_source=PIXNET&utm_medium=ppage",
   "https://aaa22034381.pixnet.net/blog/post/87537181?utm_source=PIXNET&utm_medium=ppage",
   "https://evonne1205.pixnet.net/blog/post/101978516?utm_source=PIXNET&utm_medium=ppage",
   "https://katrina1207.pixnet.net/blog/post/122059375?utm_source=PIXNET&utm_medium=ppage",
   "https://kimandmonica.pixnet.net/blog/post/92928679?utm_source=PIXNET&utm_medium=ppage",
   "https://may0708.pixnet.net/blog/post/569938760?utm_source=PIXNET&utm_medium=ppage",
   "https://twoncinpa2.pixnet.net/blog/post/364316233?utm_source=PIXNET&utm_medium=ppage",
   "https://yunny0421.pixnet.net/blog/post/337607941?utm_source=PIXNET&utm_medium=ppage",
   "https://annatree2014.pixnet.net/blog/post/356849149?utm_source=PIXNET&utm_medium=ppage",
   "https://cndjourney.pixnet.net/blog/post/338423425?utm_source=PIXNET&utm_medium=ppage",
   "https://kk961208.pixnet.net/blog/post/567606048?utm_source=PIXNET&utm_medium=ppage",
   "https://penny800302.pixnet.net/blog/post/340334238?utm_source=PIXNET&utm_medium=ppage"



      # 你可以繼續加入更多網址
    ]

    start_index = 124  # 從 articles18.csv 開始命名

    for i, url in enumerate(urls, start=start_index):
        try:
            df = scrape_pixnet_article(url)
            filename = f"articles{i}.csv"
            df.to_csv(filename, encoding="utf-8", index=False)
            print(f"✅ 儲存成功：{filename}")
        except Exception as e:
            print(f"❌ 錯誤：{url} -> {e}")

import requests
from bs4 import BeautifulSoup
import pandas as pd

# 定義爬蟲函式
def scrape_pixnet_article(url: str) -> pd.DataFrame:
    headers = {
        "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36"
    }
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.content, 'html.parser')

    # 抓標題
    title_div = soup.find('div', class_='article')
    title_tag = title_div.find('h2') if title_div else None
    title = title_tag.text.strip() if title_tag else "找不到標題"

    # 發文日期
    publish_li = soup.find('li', class_='publish')
    date = publish_li.get_text(separator=' ', strip=True) if publish_li else "找不到發文日期"

    # 抓內容
    content_div = soup.find('div', class_='article-content-inner')
    if content_div:
        p_tags = content_div.find_all('p')
        content = "\n".join(p.text.strip() for p in p_tags)
    else:
        content = "找不到內容區塊"

    return pd.DataFrame([[title, date, content]], columns=["title", "date", "content"])

# 主程式：多網址、多CSV檔案
if __name__ == "__main__":
    urls = [
   "https://pandafish2018.pixnet.net/blog/post/343879596?utm_source=PIXNET&utm_medium=ppage",
   "https://ctinas604.pixnet.net/blog/post/340574949?utm_source=PIXNET&utm_medium=ppage",
   "https://ctinas604.pixnet.net/blog/post/341799597?utm_source=PIXNET&utm_medium=ppage",
   "https://ctinas604.pixnet.net/blog/post/339959685?utm_source=PIXNET&utm_medium=ppage",
   "https://cc710510.pixnet.net/blog/post/572647492?utm_source=PIXNET&utm_medium=ppage",
   "https://unatsai525.pixnet.net/blog/post/572311028?utm_source=PIXNET&utm_medium=ppage",
   "https://unatsai525.pixnet.net/blog/post/571998640?utm_source=PIXNET&utm_medium=ppage",
   "https://even615.pixnet.net/blog/post/121435900?utm_source=PIXNET&utm_medium=ppage",
   "https://yanyang.pixnet.net/blog/post/232209004?utm_source=PIXNET&utm_medium=ppage",
   "https://yunny0421.pixnet.net/blog/post/336229645?utm_source=PIXNET&utm_medium=ppage",
   "https://momocowang.pixnet.net/blog/post/566759028?utm_source=PIXNET&utm_medium=ppage",
   "https://digumom.pixnet.net/blog/post/64852633?utm_source=PIXNET&utm_medium=ppage",
   "https://sic27ap.pixnet.net/blog/post/49957276?utm_source=PIXNET&utm_medium=ppage#google_vignette",
   "https://whitney03050305.pixnet.net/blog/post/354916648?utm_source=PIXNET&utm_medium=ppage",
   "https://joyiner1988.pixnet.net/blog/post/73267696?utm_source=PIXNET&utm_medium=ppage",
   "https://chichulife.pixnet.net/blog/post/225978365?utm_source=PIXNET&utm_medium=ppage",
   "https://wakabayashi.pixnet.net/blog/post/64320496?utm_source=PIXNET&utm_medium=ppage",
   "https://aaa22034381.pixnet.net/blog/post/62396833?utm_source=PIXNET&utm_medium=ppage",
   "https://minkuo.pixnet.net/blog/post/121377890?utm_source=PIXNET&utm_medium=ppage",
   "https://cy4103134.pixnet.net/blog/post/363796426?utm_source=PIXNET&utm_medium=ppage"



      # 你可以繼續加入更多網址
    ]

    start_index = 144  # 從 articles18.csv 開始命名

    for i, url in enumerate(urls, start=start_index):
        try:
            df = scrape_pixnet_article(url)
            filename = f"articles{i}.csv"
            df.to_csv(filename, encoding="utf-8", index=False)
            print(f"✅ 儲存成功：{filename}")
        except Exception as e:
            print(f"❌ 錯誤：{url} -> {e}")

import requests
from bs4 import BeautifulSoup
import pandas as pd

# 定義爬蟲函式
def scrape_pixnet_article(url: str) -> pd.DataFrame:
    headers = {
        "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36"
    }
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.content, 'html.parser')

    # 抓標題
    title_div = soup.find('div', class_='article')
    title_tag = title_div.find('h2') if title_div else None
    title = title_tag.text.strip() if title_tag else "找不到標題"

    # 發文日期
    publish_li = soup.find('li', class_='publish')
    date = publish_li.get_text(separator=' ', strip=True) if publish_li else "找不到發文日期"

    # 抓內容
    content_div = soup.find('div', class_='article-content-inner')
    if content_div:
        p_tags = content_div.find_all('p')
        content = "\n".join(p.text.strip() for p in p_tags)
    else:
        content = "找不到內容區塊"

    return pd.DataFrame([[title, date, content]], columns=["title", "date", "content"])

# 主程式：多網址、多CSV檔案
if __name__ == "__main__":
    urls = [
   "https://hayleyyu3306.pixnet.net/blog/post/221231493?utm_source=PIXNET&utm_medium=ppage",
   "https://hayleyyu3306.pixnet.net/blog/post/221366499?utm_source=PIXNET&utm_medium=ppage",
   "https://hayleyyu3306.pixnet.net/blog/post/221602176?utm_source=PIXNET&utm_medium=ppage",
   "https://kikimilk2.pixnet.net/blog/post/335397813?utm_source=PIXNET&utm_medium=ppage",
   "https://hero789456.pixnet.net/blog/post/221459301?utm_source=PIXNET&utm_medium=ppage",
   "https://even615.pixnet.net/blog/post/120998516?utm_source=PIXNET&utm_medium=ppage",
   "https://katetravel520.pixnet.net/blog/post/225192403?utm_source=PIXNET&utm_medium=ppage",
   "https://meiworld.pixnet.net/blog/post/51424978?utm_source=PIXNET&utm_medium=ppage",
   "https://aliceeeee.pixnet.net/blog/post/35187193?utm_source=PIXNET&utm_medium=ppage",
   "https://momocowang.pixnet.net/blog/post/557048950?utm_source=PIXNET&utm_medium=ppage",
   "https://ingrid0604.pixnet.net/blog/post/354459268?utm_source=PIXNET&utm_medium=ppage",
   "https://ivy50819.pixnet.net/blog/post/353592850?utm_source=PIXNET&utm_medium=ppage",
   "https://ivy50819.pixnet.net/blog/post/353530206?utm_source=PIXNET&utm_medium=ppage",
   "https://rainie0516.pixnet.net/blog/post/361081580?utm_source=PIXNET&utm_medium=ppage",
   "https://chunyu405.pixnet.net/blog/post/231307420?utm_source=PIXNET&utm_medium=ppage",
   "https://aaa22034381.pixnet.net/blog/post/40833553?utm_source=PIXNET&utm_medium=ppage",
   "https://even615.pixnet.net/blog/post/121081186?utm_source=PIXNET&utm_medium=ppage",
   "https://kikimilk2.pixnet.net/blog/post/334337296?utm_source=PIXNET&utm_medium=ppage",
   "https://may0708.pixnet.net/blog/post/469300985?utm_source=PIXNET&utm_medium=ppage",
   "https://even615.pixnet.net/blog/post/120822257?utm_source=PIXNET&utm_medium=ppage",
   "https://ivy50819.pixnet.net/blog/post/352610521?utm_source=PIXNET&utm_medium=ppage",
   "https://ivy50819.pixnet.net/blog/post/353118508?utm_source=PIXNET&utm_medium=ppage",
   "https://any3can.pixnet.net/blog/post/351826306?utm_source=PIXNET&utm_medium=ppage",
   "https://ivan1115.pixnet.net/blog/post/45543343?utm_source=PIXNET&utm_medium=ppage"




      # 你可以繼續加入更多網址
    ]

    start_index = 164  # 從 articles18.csv 開始命名

    for i, url in enumerate(urls, start=start_index):
        try:
            df = scrape_pixnet_article(url)
            filename = f"articles{i}.csv"
            df.to_csv(filename, encoding="utf-8", index=False)
            print(f"✅ 儲存成功：{filename}")
        except Exception as e:
            print(f"❌ 錯誤：{url} -> {e}")

import requests
from bs4 import BeautifulSoup
import pandas as pd

# 定義爬蟲函式
def scrape_pixnet_article(url: str) -> pd.DataFrame:
    headers = {
        "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36"
    }
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.content, 'html.parser')

    # 抓標題
    title_div = soup.find('div', class_='article')
    title_tag = title_div.find('h2') if title_div else None
    title = title_tag.text.strip() if title_tag else "找不到標題"

    # 發文日期
    publish_li = soup.find('li', class_='publish')
    date = publish_li.get_text(separator=' ', strip=True) if publish_li else "找不到發文日期"

    # 抓內容
    content_div = soup.find('div', class_='article-content-inner')
    if content_div:
        p_tags = content_div.find_all('p')
        content = "\n".join(p.text.strip() for p in p_tags)
    else:
        content = "找不到內容區塊"

    return pd.DataFrame([[title, date, content]], columns=["title", "date", "content"])

# 主程式：多網址、多CSV檔案
if __name__ == "__main__":
    urls = [
   "https://ivan1115.pixnet.net/blog/post/19032551?utm_source=PIXNET&utm_medium=ppage",
   "https://ivan1115.pixnet.net/blog/post/43412341?utm_source=PIXNET&utm_medium=ppage",
   "https://ivan1115.pixnet.net/blog/post/44449504?utm_source=PIXNET&utm_medium=ppage",
   "https://ivan1115.pixnet.net/blog/post/19042661?utm_source=PIXNET&utm_medium=ppage",
   "https://nikitarh.pixnet.net/blog/post/5809681?utm_source=PIXNET&utm_medium=ppage",
   "https://aiting1129.pixnet.net/blog/post/322878578?utm_source=PIXNET&utm_medium=ppage",
   "https://pei95416.pixnet.net/blog/post/360167385?utm_source=PIXNET&utm_medium=ppage",
   "https://pei95416.pixnet.net/blog/post/360611918?utm_source=PIXNET&utm_medium=ppage",
   "https://vivianchang0718.pixnet.net/blog/post/469540721?utm_source=PIXNET&utm_medium=ppage",
   "https://henrychen1974.pixnet.net/blog/post/463671962?utm_source=PIXNET&utm_medium=ppage",
   "https://tctony2222.pixnet.net/blog/post/308301182?utm_source=PIXNET&utm_medium=ppage",
   "https://bbpeng2.pixnet.net/blog/post/228592643?utm_source=PIXNET&utm_medium=ppage",
   "https://bbpeng2.pixnet.net/blog/post/228441404?utm_source=PIXNET&utm_medium=ppage",
   "https://avondiary.pixnet.net/blog/post/223265895?utm_source=PIXNET&utm_medium=ppage",
   "https://lilychen1128.pixnet.net/blog/post/159670262?utm_source=PIXNET&utm_medium=ppage",
   "https://bbpeng2.pixnet.net/blog/post/224178227?utm_source=PIXNET&utm_medium=ppage",
   "https://paicj.pixnet.net/blog/post/400865378?utm_source=PIXNET&utm_medium=ppage",
   "https://evshhips.pixnet.net/blog/post/271898971?utm_source=PIXNET&utm_medium=ppage"




      # 你可以繼續加入更多網址
    ]

    start_index = 188  # 從 articles18.csv 開始命名

    for i, url in enumerate(urls, start=start_index):
        try:
            df = scrape_pixnet_article(url)
            filename = f"articles{i}.csv"
            df.to_csv(filename, encoding="utf-8", index=False)
            print(f"✅ 儲存成功：{filename}")
        except Exception as e:
            print(f"❌ 錯誤：{url} -> {e}")

import requests
from bs4 import BeautifulSoup
import pandas as pd

# 定義爬蟲函式
def scrape_pixnet_article(url: str) -> pd.DataFrame:
    headers = {
        "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36"
    }
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.content, 'html.parser')

    # 抓標題
    title_div = soup.find('div', class_='article')
    title_tag = title_div.find('h2') if title_div else None
    title = title_tag.text.strip() if title_tag else "找不到標題"

    # 發文日期
    publish_li = soup.find('li', class_='publish')
    date = publish_li.get_text(separator=' ', strip=True) if publish_li else "找不到發文日期"

    # 抓內容
    content_div = soup.find('div', class_='article-content-inner')
    if content_div:
        p_tags = content_div.find_all('p')
        content = "\n".join(p.text.strip() for p in p_tags)
    else:
        content = "找不到內容區塊"

    return pd.DataFrame([[title, date, content]], columns=["title", "date", "content"])

# 主程式：多網址、多CSV檔案
if __name__ == "__main__":
    urls = [
   "https://a0915900120.pixnet.net/blog/post/366912643?utm_source=PIXNET&utm_medium=ppage",
   "https://paicj.pixnet.net/blog/post/407190357?utm_source=PIXNET&utm_medium=ppage",
   "https://vanessarou.pixnet.net/blog/post/176696575?utm_source=PIXNET&utm_medium=ppage",
   "https://paicj.pixnet.net/blog/post/407161029?utm_source=PIXNET&utm_medium=ppage",
   "https://kikicoco5.pixnet.net/blog/post/122550170?utm_source=PIXNET&utm_medium=ppage",
   "https://paicj.pixnet.net/blog/post/407125781?utm_source=PIXNET&utm_medium=ppage",
   "https://paicj.pixnet.net/blog/post/407089745?utm_source=PIXNET&utm_medium=ppage",
   "https://livi1233.pixnet.net/blog/post/346080981?utm_source=PIXNET&utm_medium=ppage",
   "https://ajhomom.pixnet.net/blog/post/166541563?utm_source=PIXNET&utm_medium=ppage",
   "https://flowermei713.pixnet.net/blog/post/577147672?utm_source=PIXNET&utm_medium=ppage",
   "https://ajhomom.pixnet.net/blog/post/164343472?utm_source=PIXNET&utm_medium=ppage",
   "https://jack74327.pixnet.net/blog/post/71319535?utm_source=PIXNET&utm_medium=ppage",
   "https://paicj.pixnet.net/blog/post/407231213?utm_source=PIXNET&utm_medium=ppage",
   "https://aaa22034381.pixnet.net/blog/post/181447807?utm_source=PIXNET&utm_medium=ppage"





      # 你可以繼續加入更多網址
    ]

    start_index = 206  # 從 articles18.csv 開始命名

    for i, url in enumerate(urls, start=start_index):
        try:
            df = scrape_pixnet_article(url)
            filename = f"articles{i}.csv"
            df.to_csv(filename, encoding="utf-8", index=False)
            print(f"✅ 儲存成功：{filename}")
        except Exception as e:
            print(f"❌ 錯誤：{url} -> {e}")

import requests
from bs4 import BeautifulSoup
import pandas as pd

# 定義爬蟲函式
def scrape_pixnet_article(url: str) -> pd.DataFrame:
    headers = {
        "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36"
    }
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.content, 'html.parser')

    # 抓標題
    title_div = soup.find('div', class_='article')
    title_tag = title_div.find('h2') if title_div else None
    title = title_tag.text.strip() if title_tag else "找不到標題"

    # 發文日期
    publish_li = soup.find('li', class_='publish')
    date = publish_li.get_text(separator=' ', strip=True) if publish_li else "找不到發文日期"

    # 抓內容
    content_div = soup.find('div', class_='article-content-inner')
    if content_div:
        p_tags = content_div.find_all('p')
        content = "\n".join(p.text.strip() for p in p_tags)
    else:
        content = "找不到內容區塊"

    return pd.DataFrame([[title, date, content]], columns=["title", "date", "content"])

# 主程式：多網址、多CSV檔案
if __name__ == "__main__":
    urls = [
  "https://paicj.pixnet.net/blog/post/407041145?utm_source=PIXNET&utm_medium=ppage",
  "https://hardaway.com.tw/blog/post/49349192?utm_source=PIXNET&utm_medium=ppage",
  "https://carrieok0925.pixnet.net/blog/post/162353272?utm_source=PIXNET&utm_medium=ppage",
  "https://paicj.pixnet.net/blog/post/407029537?utm_source=PIXNET&utm_medium=ppage",
  "https://evshhips.pixnet.net/blog/post/357607860?utm_source=PIXNET&utm_medium=ppage",
  "https://alrena.pixnet.net/blog/post/234145656?utm_source=PIXNET&utm_medium=ppage",
  "https://evshhips.pixnet.net/blog/post/357498300?utm_source=PIXNET&utm_medium=ppage",
  "https://wtw332589.pixnet.net/blog/post/345925543?utm_source=PIXNET&utm_medium=ppage",
  "https://evshhips.pixnet.net/blog/post/357474357?utm_source=PIXNET&utm_medium=ppage",
  "https://mstravel20.pixnet.net/blog/post/157958401?utm_source=PIXNET&utm_medium=ppage",
  "https://v84454058.pixnet.net/blog/post/357450765?utm_source=PIXNET&utm_medium=ppage#google_vignette",
  "https://evshhips.pixnet.net/blog/post/357431142?utm_source=PIXNET&utm_medium=ppage",
  "https://little15.pixnet.net/blog/post/50869564?utm_source=PIXNET&utm_medium=ppage",
  "https://evshhips.pixnet.net/blog/post/357409563?utm_source=PIXNET&utm_medium=ppage",
  "https://mei30530.pixnet.net/blog/post/576646692?utm_source=PIXNET&utm_medium=ppage",
  "https://monkeybowbi.pixnet.net/blog/post/576648844?utm_source=PIXNET&utm_medium=ppage",
  "https://evonne1205.pixnet.net/blog/post/152523736?utm_source=PIXNET&utm_medium=ppage",
  "https://campingshark86.pixnet.net/blog/post/152292595?utm_source=PIXNET&utm_medium=ppage",
  "https://mei30530.pixnet.net/blog/post/576542712?utm_source=PIXNET&utm_medium=ppage",
  "https://sammi0224.pixnet.net/blog/post/122328713?utm_source=PIXNET&utm_medium=ppage",
  "https://katrina1207.pixnet.net/blog/post/150030244?utm_source=PIXNET&utm_medium=ppage",
  "https://aaa22034381.pixnet.net/blog/post/142558195?utm_source=PIXNET&utm_medium=ppage",
  "https://yunjie059.pixnet.net/blog/post/149460637?utm_source=PIXNET&utm_medium=ppage",
  "https://bellwort9.pixnet.net/blog/post/71167279?utm_source=PIXNET&utm_medium=ppage"






      # 你可以繼續加入更多網址
    ]

    start_index = 220  # 從 articles18.csv 開始命名

    for i, url in enumerate(urls, start=start_index):
        try:
            df = scrape_pixnet_article(url)
            filename = f"articles{i}.csv"
            df.to_csv(filename, encoding="utf-8", index=False)
            print(f"✅ 儲存成功：{filename}")
        except Exception as e:
            print(f"❌ 錯誤：{url} -> {e}")