# -*- coding: utf-8 -*-
"""Tibameå°ˆé¡Œ.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LiFxLlxUo5a0LOPyFwfbixHh9Hv1pxg_

# è¼‰å…¥å¿…è¦å¥—ä»¶
"""

!pip install requests beautifulsoup4

import requests
from bs4 import BeautifulSoup

url = "https://paicj.pixnet.net/blog/post/407084009"
headers = {
    "User-Agent": "Mozilla/5.0"
}
response = requests.get(url, headers=headers)
response.encoding = 'utf-8'

soup = BeautifulSoup(response.text, 'html.parser')

title_tag = soup.find('h2', class_='article-title')
title = title_tag.get_text(strip=True) if title_tag else 'æ‰¾ä¸åˆ°æ¨™é¡Œ'

for h2 in soup.find_all('h2'):
    a_tag = h2.find('a')
    if a_tag and a_tag.has_attr('href'):
        print("æ¨™é¡Œï¼š", a_tag.get_text(strip=True))
        print("é€£çµï¼š", a_tag['href'])

content_div = soup.find('div', class_='article-content-inner')
if content_div:
    paragraphs = content_div.find_all('p')
    content = '\n'.join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))
else:
    content = 'æ‰¾ä¸åˆ°å…§å®¹'


print("\nå…§å®¹ï¼š\n", content)

import requests
from bs4 import BeautifulSoup

# 1. ç™¼é€ GET è«‹æ±‚
url = "https://paicj.pixnet.net/blog/post/406882969"
headers = {
    "User-Agent": "Mozilla/5.0"
}
response = requests.get(url, headers=headers)
response.encoding = 'utf-8'

# 2. è§£æ HTML
soup = BeautifulSoup(response.text, 'html.parser')

# 3. æ“·å–æ¨™é¡Œ
title_tag = soup.find('h2', class_='article-title')
title = title_tag.get_text(strip=True) if title_tag else 'æ‰¾ä¸åˆ°æ¨™é¡Œ'

for h2 in soup.find_all('h2'):
    a_tag = h2.find('a')
    if a_tag and a_tag.has_attr('href'):
        print("æ¨™é¡Œï¼š", a_tag.get_text(strip=True))
        print("é€£çµï¼š", a_tag['href'])

# 4. æ“·å–å…§å®¹
content_div = soup.find('div', class_='article-content-inner')
if content_div:
    paragraphs = content_div.find_all('p')
    content = '\n'.join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))
else:
    content = 'æ‰¾ä¸åˆ°å…§å®¹'

# 5. é¡¯ç¤ºçµæœ
print("\nå…§å®¹ï¼š\n", content)

import requests
from bs4 import BeautifulSoup

# 1. ç™¼é€ GET è«‹æ±‚
url = "https://paicj.pixnet.net/blog/post/406882969"
headers = {
    "User-Agent": "Mozilla/5.0"
}
response = requests.get(url, headers=headers)
response.encoding = 'utf-8'

# 2. è§£æ HTML
soup = BeautifulSoup(response.text, 'html.parser')

# 3. æ“·å–æ¨™é¡Œ
title_tag = soup.find('h2', class_='article-title')
title = title_tag.get_text(strip=True) if title_tag else 'æ‰¾ä¸åˆ°æ¨™é¡Œ'

for h2 in soup.find_all('h2'):
    a_tag = h2.find('a')
    if a_tag and a_tag.has_attr('href'):
        print("æ¨™é¡Œï¼š", a_tag.get_text(strip=True))
        print("é€£çµï¼š", a_tag['href'])

# 4. æ“·å–å…§å®¹
content_div = soup.find('div', class_='article-content-inner')
if content_div:
    paragraphs = content_div.find_all('p')
    content = '\n'.join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))
else:
    content = 'æ‰¾ä¸åˆ°å…§å®¹'

# 5. é¡¯ç¤ºçµæœ
print("\nå…§å®¹ï¼š\n", content)

import requests
from bs4 import BeautifulSoup

# Medium æ–‡ç«  URL
url = "https://medium.com/æ—…è¡Œä¹‹åœ°/å‹•æ©Ÿ-dbe90d4d737c"

# è¨­å®š headers æ¨¡æ“¬ç€è¦½å™¨è¡Œç‚º
headers = {
    "User-Agent": "Mozilla/5.0"
}

# ç™¼é€ GET è«‹æ±‚
response = requests.get(url, headers=headers)

# æª¢æŸ¥è«‹æ±‚æ˜¯å¦æˆåŠŸ
if response.status_code == 200:
    # è§£æ HTML å…§å®¹
    soup = BeautifulSoup(response.text, "html.parser")

    # å˜—è©¦æŠ“å–æ–‡ç« æ¨™é¡Œ
    title_tag = soup.find("h1")
    title = title_tag.get_text(strip=True) if title_tag else "ç„¡æ¨™é¡Œ"

    # å˜—è©¦æŠ“å–æ–‡ç« å…§å®¹
    paragraphs = soup.find_all("p")
    content = "\n".join(p.get_text(strip=True) for p in paragraphs)

    # è¼¸å‡ºçµæœ
    print(f"æ¨™é¡Œï¼š{title}\n")
    print("å…§å®¹ï¼š")
    print(content)
else:
    print(f"è«‹æ±‚å¤±æ•—ï¼Œç‹€æ…‹ç¢¼ï¼š{response.status_code}")

import requests
from bs4 import BeautifulSoup

# Medium æ–‡ç«  URL
url = "https://medium.com/@angela-ye-zhijunye/zjs-nature-diary-1-let-s-go-camping-%E8%8B%97%E6%A0%97%E9%9C%B2%E7%87%9F%E5%8D%80-%E5%8F%B8%E9%A6%AC%E9%99%90%E9%9B%B2%E7%AB%AF-%E5%B9%B3%E6%97%A5%E5%B0%8F%E7%A2%BA%E5%B9%B8-%E7%AC%AC%E4%B8%80%E6%AC%A1%E5%B0%B1%E6%88%90%E5%8A%9F%E7%9A%84%E9%9B%99%E4%BA%BA%E9%9C%B2%E7%87%9F-873cbd38d5b6"

# è¨­å®š headers æ¨¡æ“¬ç€è¦½å™¨è¡Œç‚º
headers = {
    "User-Agent": "Mozilla/5.0"
}

# ç™¼é€ GET è«‹æ±‚
response = requests.get(url, headers=headers)

# æª¢æŸ¥è«‹æ±‚æ˜¯å¦æˆåŠŸ
if response.status_code == 200:
    # è§£æ HTML å…§å®¹
    soup = BeautifulSoup(response.text, "html.parser")

    # å˜—è©¦æŠ“å–æ–‡ç« æ¨™é¡Œ
    title_tag = soup.find("h1")
    title = title_tag.get_text(strip=True) if title_tag else "ç„¡æ¨™é¡Œ"

    # å˜—è©¦æŠ“å–æ–‡ç« å…§å®¹
    paragraphs = soup.find_all("p")
    content = "\n".join(p.get_text(strip=True) for p in paragraphs)

    # è¼¸å‡ºçµæœ
    print(f"æ¨™é¡Œï¼š{title}\n")
    print("å…§å®¹ï¼š")
    print(content)
else:
    print(f"è«‹æ±‚å¤±æ•—ï¼Œç‹€æ…‹ç¢¼ï¼š{response.status_code}")

import requests
from bs4 import BeautifulSoup

# 1. ç™¼é€ GET è«‹æ±‚
url = "https://mumu416.pixnet.net/blog/post/161886901?utm_source=chatgpt.com"
headers = {
    "User-Agent": "Mozilla/5.0"
}
response = requests.get(url, headers=headers)
response.encoding = 'utf-8'

# 2. è§£æ HTML
soup = BeautifulSoup(response.text, 'html.parser')

# 3. æ“·å–æ¨™é¡Œ
title_tag = soup.find('h2', class_='article-title')
title = title_tag.get_text(strip=True) if title_tag else 'æ‰¾ä¸åˆ°æ¨™é¡Œ'

for h2 in soup.find_all('h2'):
    a_tag = h2.find('a')
    if a_tag and a_tag.has_attr('href'):
        print("æ¨™é¡Œï¼š", a_tag.get_text(strip=True))
        print("é€£çµï¼š", a_tag['href'])

# 4. æ“·å–å…§å®¹
content_div = soup.find('div', class_='article-content-inner')
if content_div:
    paragraphs = content_div.find_all('p')
    content = '\n'.join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))
else:
    content = 'æ‰¾ä¸åˆ°å…§å®¹'

# 5. é¡¯ç¤ºçµæœ
print("\nå…§å®¹ï¼š\n", content)

import requests
from bs4 import BeautifulSoup

# 1. ç™¼é€ GET è«‹æ±‚
url = "https://sujungyuo970926.pixnet.net/blog/post/345928714?utm_source=chatgpt.com"
headers = {
    "User-Agent": "Mozilla/5.0"
}
response = requests.get(url, headers=headers)
response.encoding = 'utf-8'

# 2. è§£æ HTML
soup = BeautifulSoup(response.text, 'html.parser')

# 3. æ“·å–æ¨™é¡Œ
title_tag = soup.find('h2', class_='article-title')
title = title_tag.get_text(strip=True) if title_tag else 'æ‰¾ä¸åˆ°æ¨™é¡Œ'

for h2 in soup.find_all('h2'):
    a_tag = h2.find('a')
    if a_tag and a_tag.has_attr('href'):
        print("æ¨™é¡Œï¼š", a_tag.get_text(strip=True))
        print("é€£çµï¼š", a_tag['href'])

# 4. æ“·å–å…§å®¹
content_div = soup.find('div', class_='article-content-inner')
if content_div:
    paragraphs = content_div.find_all('p')
    content = '\n'.join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))
else:
    content = 'æ‰¾ä¸åˆ°å…§å®¹'

# 5. é¡¯ç¤ºçµæœ
print("\nå…§å®¹ï¼š\n", content)

import requests
from bs4 import BeautifulSoup

# 1. ç™¼é€ GET è«‹æ±‚
url = "https://helensdiary.pixnet.net/blog/post/366606154?utm_source=PIXNET&utm_medium=ppage"
headers = {
    "User-Agent": "Mozilla/5.0"
}
response = requests.get(url, headers=headers)
response.encoding = 'utf-8'

# 2. è§£æ HTML
soup = BeautifulSoup(response.text, 'html.parser')

# 3. æ“·å–æ¨™é¡Œ
title_tag = soup.find('h2', class_='article-title')
title = title_tag.get_text(strip=True) if title_tag else 'æ‰¾ä¸åˆ°æ¨™é¡Œ'

for h2 in soup.find_all('h2'):
    a_tag = h2.find('a')
    if a_tag and a_tag.has_attr('href'):
        print("æ¨™é¡Œï¼š", a_tag.get_text(strip=True))
        print("é€£çµï¼š", a_tag['href'])

# 4. æ“·å–å…§å®¹
content_div = soup.find('div', class_='article-content-inner')
if content_div:
    paragraphs = content_div.find_all('p')
    content = '\n'.join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))
else:
    content = 'æ‰¾ä¸åˆ°å…§å®¹'

# 5. é¡¯ç¤ºçµæœ
print("\nå…§å®¹ï¼š\n", content)

import requests
from bs4 import BeautifulSoup


def main():
    url = "https://markandhazyl.com/camping-in-miaoli/"
    # æœ‰äº›é˜²çˆ¬èŸ²ç¶²ç«™æœƒæª¢æŸ¥è«‹æ±‚æ¨™é ­ (request headers) çš„user-agentæ˜¯å¦æœ‰å€¼ï¼Œä»¥è¾¨è­˜æ˜¯ä¸€èˆ¬ä½¿ç”¨è€…é‚„æ˜¯çˆ¬èŸ²ç¨‹å¼è¨ªå•ï¼Œä¾†æ±ºå®šæ˜¯å¦æ‹’çµ•è«‹æ±‚
    # å»ºè­°åŠ ä¸Šuser-agentï¼Œå€¼å¯ä»¥è¤‡è£½Chrome > F12 > Network > Headers > Request Headers > user-agentï¼Œä¾†å½è£æˆä¸€èˆ¬ä½¿ç”¨è€…
    # The headers variable should be a dictionary, not a set.
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36"
    }
    response = requests.get(url, headers=headers)
    if response.status_code != 200:
        print(f"è«‹æ±‚å¤±æ•—ï¼Œstatus code: {response.status_code}")
        return
    # é¡¯ç¤ºçµæœç¶²é å…§å®¹
    # print(response.text)
    # å»ºç«‹BeautifulSoupç‰©ä»¶
    soup = BeautifulSoup(response.text, "html.parser")
    # å…ˆä½¿ç”¨Chromeè§€å¯Ÿä¸€ç¯‡æ–‡ç« ç¯„åœï¼Œä¸¦ç”¨select_one()æ¸¬è©¦
    article = soup.find('h3')
    # print(f"article:\n{article}")
    # å–å‡ºæ¨™é¡Œ
    title = article.get_text(strip=True)
    # print(f"title: {title}")
    # å–å‡ºé€£çµ
    # ç•¶æ¨™é¡Œç‚ºã€Œæœ¬æ–‡å·²è¢«åˆªé™¤ã€å‰‡<a>ç‚ºç©ºå€¼
    a = article.select_one(".title > a")
    # print(f"a: {a}")
    link = a["href"] if a else None
    # å–å‡ºæ—¥æœŸ
    time_tag = soup.find('time')
    date = time_tag.get_text(strip=True)
    print(f"{title}\t{link}\n{date}")


if __name__ == "__main__":
    print("===================================")
    main()
    print("===================================")

import requests
from bs4 import BeautifulSoup

url = 'https://markandhazyl.com/camping-in-miaoli/'
response = requests.get(url)
response.encoding = 'utf-8'
soup = BeautifulSoup(response.text, 'html.parser')

# æŠ“å‡ºæ‰€æœ‰ h3 + p çµ„åˆ
h3_tags = soup.find_all('h3')
for h3 in h3_tags:
    h3_text = h3.get_text(strip=True)
    next_p = h3.find_next_sibling('p')
    if next_p:
        p_text = next_p.get_text(strip=True)
        print(f"ğŸ“ {h3_text}\nâ¡ {p_text}\n")

"""#ç¬¬ä¸€ç¯‡æ–‡ç« (æ ¼å¼ä¸åŒ)"""

import requests
from bs4 import BeautifulSoup
import pandas as pd


def getPageContent():

    url = "https://markandhazyl.com/camping-in-miaoli/"
    headers = {
        "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36"}
    response = requests.get(url, headers=headers)
    if response.status_code != 200:
        print(f"è«‹æ±‚å¤±æ•—ï¼Œstatus code: {response.status_code}")
        return
    # å»ºç«‹BeautifulSoupç‰©ä»¶
    soup = BeautifulSoup(response.text, "html.parser")
    articles = []

    level1 = soup.select_one("div.entry-content.single-content")
    level2_title = level1.select("a > strong")[:17]
    # print(level2_title)
    for level3 in level2_title:
      title = level3.text.strip()
      link = level3.parent.get("href")
      level3_block = level3.find_parent("h3").next_siblings
      content = ""
      for p in level3_block:
        if p.name == "h3":
          break
        content += p.text

      #content = level3_block.find_next_sibling('p').get_text(strip=True)
      articles.append([title,link,content])

    # å°‡æ•´é æ‰€æœ‰æ–‡ç« è½‰å­˜æ”¾è‡³DataFrame
    df = pd.DataFrame(articles, columns=["title", "link", "content"])
    return df

def dataInfo(df):
    # åˆ—å‡ºDataFrameè³‡è¨Šï¼Œä»¥æŸ¥è©¢å“ªäº›æ¬„ä½æœ‰ç©ºå€¼
    df.info()
    print(df)


def main():
    df = getPageContent()
    dataInfo(df)
    df.to_csv("articles.csv",encoding="utf-8")


if __name__ == "__main__":

    main()

"""# ç¬¬äºŒç¯‡æ–‡ç« """

import requests
from bs4 import BeautifulSoup
import pandas as pd

url = "https://qqkelly.com/kaohuing/"
headers = {
    "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36"
}
response = requests.get(url, headers=headers)
soup = BeautifulSoup(response.content, 'html.parser')
articles=[]

# æŠ“æ¨™é¡Œ
title_div = soup.find('div', class_='post-header')
title_tag = title_div.find('h1')
title = title_tag.text.strip()

# æŠ“ç™¼æ–‡æ—¥æœŸ
date_tag = title_div.find('span', class_='post-date')
date = date_tag.text.strip()

# æŠ“å…§å®¹
content_div = soup.find('div', class_='post-entry')
p_tags = content_div.find_all('p')
content = "\n".join(p.text.strip() for p in p_tags)  # æŠŠæ‰€æœ‰æ®µè½åˆä½µæˆä¸€æ®µæ–‡å­—ï¼Œæ›è¡Œåˆ†é–‹


articles.append([title,date,content])

df = pd.DataFrame(articles, columns=["title", "date", "content"])

# æ”¾é€²DataFrame
df.info()
print(df)

df.to_csv("articles2.csv",encoding="utf-8")

"""# ç¬¬ä¸‰ç¯‡æ–‡ç« """

import requests
from bs4 import BeautifulSoup
import pandas as pd

url = "https://fumtravel.pixnet.net/blog/post/181212259?utm_source=PIXNET&utm_medium=ppage"
headers = {
    "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36"
}
response = requests.get(url, headers=headers)
soup = BeautifulSoup(response.content, 'html.parser')
articles=[]

# æŠ“æ¨™é¡Œ
title_div = soup.find('div', class_='article')
title_tag = title_div.find('h2')
title = title_tag.text.strip()
print(title)

#ç™¼æ–‡æ—¥æœŸ
publish_li = soup.find('li', class_='publish')
if publish_li:
    text = publish_li.get_text(separator=' ', strip=True)
    print(text)
else:
    print("æ‰¾ä¸åˆ° publish çš„ li")



# æŠ“å…§å®¹
content_div = soup.find('div', class_='article-content-inner')
p_tags = content_div.find_all('p')
content = "\n".join(p.text.strip() for p in p_tags)



articles.append([title,text,content])

df = pd.DataFrame(articles, columns=["title", "date", "content"])

# æ”¾é€²DataFrame
df.info()
print(df)

df.to_csv("articles3.csv",encoding="utf-8")

"""# ç¬¬å››ç¯‡æ–‡ç« (å°‘äº†æ—¥æœŸ)"""

import requests
from bs4 import BeautifulSoup
import pandas as pd

url = "https://dpmm2021.pixnet.net/blog/post/67557052?utm_source=PIXNET&utm_medium=ppage"
headers = {
    "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36"
}
response = requests.get(url, headers=headers)
soup = BeautifulSoup(response.content, 'html.parser')
articles = []

# æŠ“æ¨™é¡Œ
title_div = soup.find('div', class_='article')
title_tag = title_div.find('h2')
title = title_tag.text.strip()
print(title)

# ç™¼æ–‡æ—¥æœŸ
publish_li = soup.find('li', class_='publish')
if publish_li:
    text = publish_li.get_text(separator=' ', strip=True)
    print(text)
else:
    print("æ‰¾ä¸åˆ° publish çš„ li")

content = ""
h4_tags = soup.select('h4')
# é€ä¸€æŠ“å–h4æ¨™ç±¤ä¸‹çš„spanæ¨™ç±¤ï¼Œå†æŠ“è£¡é¢çš„strong
for h4 in h4_tags[:96]:
    span_tag = h4.find('span')  # æ‰¾åˆ°h4ä¸‹çš„span
    # Change is here. Instead of searching for a parent, iterate through siblings directly
    level3_block = h4.next_siblings
    content = ""
    for p in level3_block:
        if p.name == "h3":
            break
        if p.name == 'p': # Only consider <p> tags for content
            content += p.text.strip()
    if span_tag:
        strong_tag = span_tag.find('strong').text.strip()  # æ‰¾åˆ°spanä¸‹çš„strong
        #print(strong_tag.text.strip())  # è¼¸å‡ºstrongçš„å…§å®¹

    articles.append([strong_tag, content])  # Append content, not p_tags


df = pd.DataFrame(articles, columns=["title", "content"])

# æ”¾é€²DataFrame
df.info()
print(df)

df.to_csv("articles4.csv", encoding="utf-8")

"""# ç¬¬äº”ç¯‡æ–‡ç« """

import requests
from bs4 import BeautifulSoup
import pandas as pd

url = "https://sujungyuo970926.pixnet.net/blog/post/347228242?utm_source=PIXNET&utm_medium=ppage"
headers = {
    "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36"
}
response = requests.get(url, headers=headers)
soup = BeautifulSoup(response.content, 'html.parser')
articles = []

# æŠ“æ¨™é¡Œ
title_div = soup.find('div', class_='article')
title_tag = title_div.find('h2')
title = title_tag.text.strip()
print(title)

# ç™¼æ–‡æ—¥æœŸ
publish_li = soup.find('li', class_='publish')
if publish_li:
    text = publish_li.get_text(separator=' ', strip=True)
    print(text)
else:
    print("æ‰¾ä¸åˆ° publish çš„ li")

content = ""
content_div = soup.find('div', class_='article-content-inner')
p_tags = content_div.find_all('p')
content = "\n".join(p.text.strip() for p in p_tags)

articles.append([title,text,content])


df = pd.DataFrame(articles, columns=["title", "date","content"])

# æ”¾é€²DataFrame
df.info()
print(df)

df.to_csv("articles5.csv", encoding="utf-8")

"""# ç¬¬å…­ç¯‡æ–‡ç« """

import requests
from bs4 import BeautifulSoup
import pandas as pd

url = "https://sunboy0722.pixnet.net/blog/post/358228299?utm_source=PIXNET&utm_medium=ppage"
headers = {
    "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36"
}
response = requests.get(url, headers=headers)
soup = BeautifulSoup(response.content, 'html.parser')
articles = []

# æŠ“æ¨™é¡Œ
title_div = soup.find('div', class_='article')
title_tag = title_div.find('h2')
title = title_tag.text.strip()
print(title)

# ç™¼æ–‡æ—¥æœŸ
publish_li = soup.find('li', class_='publish')
if publish_li:
    text = publish_li.get_text(separator=' ', strip=True)
    print(text)
else:
    print("æ‰¾ä¸åˆ° publish çš„ li")

content = ""
content_div = soup.find('div', class_='article-content-inner')
p_tags = content_div.find_all('p')[:213]
content = "\n".join(p.text.strip() for p in p_tags)

articles.append([title,text,content])


df = pd.DataFrame(articles, columns=["title", "date","content"])

# æ”¾é€²DataFrame
df.info()
print(df)

df.to_csv("articles6.csv", encoding="utf-8")

"""# ç¬¬ä¸ƒç¯‡æ–‡ç« """

import requests
from bs4 import BeautifulSoup
import pandas as pd

url = "https://abc0601.pixnet.net/blog/post/347175517?utm_source=PIXNET&utm_medium=ppage"
headers = {
    "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36"
}
response = requests.get(url, headers=headers)
soup = BeautifulSoup(response.content, 'html.parser')
articles = []

# æŠ“æ¨™é¡Œ
title_div = soup.find('div', class_='article')
title_tag = title_div.find('h2')
title = title_tag.text.strip()
print(title)

# ç™¼æ–‡æ—¥æœŸ
publish_li = soup.find('li', class_='publish')
if publish_li:
    text = publish_li.get_text(separator=' ', strip=True)
    print(text)
else:
    print("æ‰¾ä¸åˆ° publish çš„ li")

content = ""
content_div = soup.find('div', class_='article-content-inner')
p_tags = content_div.find_all('p')
content = "\n".join(p.text.strip() for p in p_tags)

articles.append([title,text,content])


df = pd.DataFrame(articles, columns=["title", "date","content"])

# æ”¾é€²DataFrame
df.info()
print(df)

df.to_csv("articles7.csv", encoding="utf-8")

"""# ç¬¬å…«ç¯‡æ–‡ç« """

import requests
from bs4 import BeautifulSoup
import pandas as pd

url = "https://eeooa0314.pixnet.net/blog/post/577385944?utm_source=PIXNET&utm_medium=ppage"
headers = {
    "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36"
}
response = requests.get(url, headers=headers)
soup = BeautifulSoup(response.content, 'html.parser')
articles = []

# æŠ“æ¨™é¡Œ
title_div = soup.find('div', class_='article')
title_tag = title_div.find('h2')
title = title_tag.text.strip()
print(title)

# ç™¼æ–‡æ—¥æœŸ
publish_li = soup.find('li', class_='publish')
if publish_li:
    text = publish_li.get_text(separator=' ', strip=True)
    print(text)
else:
    print("æ‰¾ä¸åˆ° publish çš„ li")

content = ""
content_div = soup.find('div', class_='article-content-inner')
p_tags = content_div.find_all('p')
content = "\n".join(p.text.strip() for p in p_tags)

articles.append([title,text,content])


df = pd.DataFrame(articles, columns=["title", "date","content"])

# æ”¾é€²DataFrame
df.info()
print(df)

df.to_csv("articles8.csv", encoding="utf-8")

"""# ç¬¬ä¹ç¯‡æ–‡ç« """

import requests
from bs4 import BeautifulSoup
import pandas as pd

url = "https://sunboy0722.pixnet.net/blog/post/358031400?utm_source=PIXNET&utm_medium=ppage"
headers = {
    "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36"
}
response = requests.get(url, headers=headers)
soup = BeautifulSoup(response.content, 'html.parser')
articles = []

# æŠ“æ¨™é¡Œ
title_div = soup.find('div', class_='article')
title_tag = title_div.find('h2')
title = title_tag.text.strip()
print(title)

# ç™¼æ–‡æ—¥æœŸ
publish_li = soup.find('li', class_='publish')
if publish_li:
    text = publish_li.get_text(separator=' ', strip=True)
    print(text)
else:
    print("æ‰¾ä¸åˆ° publish çš„ li")

content = ""
content_div = soup.find('div', class_='article-content-inner')
p_tags = content_div.find_all('p')
content = "\n".join(p.text.strip() for p in p_tags)

articles.append([title,text,content])


df = pd.DataFrame(articles, columns=["title", "date","content"])

# æ”¾é€²DataFrame
df.info()
print(df)

df.to_csv("articles9.csv", encoding="utf-8")

"""# ç¬¬åç¯‡æ–‡ç« """

import requests
from bs4 import BeautifulSoup
import pandas as pd

url = "https://abcdef12332114.pixnet.net/blog/post/222897223?utm_source=PIXNET&utm_medium=ppage"
headers = {
    "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36"
}
response = requests.get(url, headers=headers)
soup = BeautifulSoup(response.content, 'html.parser')
articles = []

# æŠ“æ¨™é¡Œ
title_div = soup.find('div', class_='article')
title_tag = title_div.find('h2')
title = title_tag.text.strip()
print(title)

# ç™¼æ–‡æ—¥æœŸ
publish_li = soup.find('li', class_='publish')
if publish_li:
    text = publish_li.get_text(separator=' ', strip=True)
    print(text)
else:
    print("æ‰¾ä¸åˆ° publish çš„ li")

content = ""
content_div = soup.find('div', class_='article-content-inner')
p_tags = content_div.find_all('p')
content = "\n".join(p.text.strip() for p in p_tags)

articles.append([title,text,content])


df = pd.DataFrame(articles, columns=["title", "date","content"])

# æ”¾é€²DataFrame
df.info()
print(df)

df.to_csv("articles10.csv", encoding="utf-8")

"""# ç¬¬åä¸€ç¯‡æ–‡ç« """

import requests
from bs4 import BeautifulSoup
import pandas as pd

url = "https://sujungyuo970926.pixnet.net/blog/post/346623334?utm_source=PIXNET&utm_medium=ppage"
headers = {
    "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36"
}
response = requests.get(url, headers=headers)
soup = BeautifulSoup(response.content, 'html.parser')
articles = []

# æŠ“æ¨™é¡Œ
title_div = soup.find('div', class_='article')
title_tag = title_div.find('h2')
title = title_tag.text.strip()
print(title)

# ç™¼æ–‡æ—¥æœŸ
publish_li = soup.find('li', class_='publish')
if publish_li:
    text = publish_li.get_text(separator=' ', strip=True)
    print(text)
else:
    print("æ‰¾ä¸åˆ° publish çš„ li")

content = ""
content_div = soup.find('div', class_='article-content-inner')
p_tags = content_div.find_all('p')
content = "\n".join(p.text.strip() for p in p_tags)

articles.append([title,text,content])


df = pd.DataFrame(articles, columns=["title", "date","content"])

# æ”¾é€²DataFrame
df.info()
print(df)

df.to_csv("articles11.csv", encoding="utf-8")

"""# ç¬¬åäºŒç¯‡æ–‡ç« """

import requests
from bs4 import BeautifulSoup
import pandas as pd

url = "https://paicj.pixnet.net/blog/post/407084009?utm_source=PIXNET&utm_medium=ppage"
headers = {
    "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36"
}
response = requests.get(url, headers=headers)
soup = BeautifulSoup(response.content, 'html.parser')
articles = []

# æŠ“æ¨™é¡Œ
title_div = soup.find('div', class_='article')
title_tag = title_div.find('h2')
title = title_tag.text.strip()
print(title)

# ç™¼æ–‡æ—¥æœŸ
publish_li = soup.find('li', class_='publish')
if publish_li:
    text = publish_li.get_text(separator=' ', strip=True)
    print(text)
else:
    print("æ‰¾ä¸åˆ° publish çš„ li")

content = ""
content_div = soup.find('div', class_='article-content-inner')
p_tags = content_div.find_all('p')
content = "\n".join(p.text.strip() for p in p_tags)

articles.append([title,text,content])


df = pd.DataFrame(articles, columns=["title", "date","content"])

# æ”¾é€²DataFrame
df.info()
print(df)

df.to_csv("articles12.csv", encoding="utf-8")

"""# ç¬¬åä¸‰ç¯‡æ–‡ç« """

import requests
from bs4 import BeautifulSoup
import pandas as pd

url = "https://evonne1205.pixnet.net/blog/post/165797605?utm_source=PIXNET&utm_medium=ppage"
headers = {
    "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36"
}
response = requests.get(url, headers=headers)
soup = BeautifulSoup(response.content, 'html.parser')
articles = []

# æŠ“æ¨™é¡Œ
title_div = soup.find('div', class_='article')
title_tag = title_div.find('h2')
title = title_tag.text.strip()
print(title)

# ç™¼æ–‡æ—¥æœŸ
publish_li = soup.find('li', class_='publish')
if publish_li:
    text = publish_li.get_text(separator=' ', strip=True)
    print(text)
else:
    print("æ‰¾ä¸åˆ° publish çš„ li")

content = ""
content_div = soup.find('div', class_='article-content-inner')
p_tags = content_div.find_all('p')
content = "\n".join(p.text.strip() for p in p_tags)

articles.append([title,text,content])


df = pd.DataFrame(articles, columns=["title", "date","content"])

# æ”¾é€²DataFrame
df.info()
print(df)

df.to_csv("articles13.csv", encoding="utf-8")

"""# ç¬¬åå››ç¯‡æ–‡ç« """

import requests
from bs4 import BeautifulSoup
import pandas as pd

url = "https://cycle1003.pixnet.net/blog/post/163937233?utm_source=PIXNET&utm_medium=ppage"
headers = {
    "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36"
}
response = requests.get(url, headers=headers)
soup = BeautifulSoup(response.content, 'html.parser')
articles = []

# æŠ“æ¨™é¡Œ
title_div = soup.find('div', class_='article')
title_tag = title_div.find('h2')
title = title_tag.text.strip()
print(title)

# ç™¼æ–‡æ—¥æœŸ
publish_li = soup.find('li', class_='publish')
if publish_li:
    text = publish_li.get_text(separator=' ', strip=True)
    print(text)
else:
    print("æ‰¾ä¸åˆ° publish çš„ li")

content = ""
content_div = soup.find('div', class_='article-content-inner')
p_tags = content_div.find_all('p')
content = "\n".join(p.text.strip() for p in p_tags)

articles.append([title,text,content])


df = pd.DataFrame(articles, columns=["title", "date","content"])

# æ”¾é€²DataFrame
df.info()
print(df)

df.to_csv("articles14.csv", encoding="utf-8")

"""# ç¬¬åäº”ç¯‡æ–‡ç« """

import requests
from bs4 import BeautifulSoup
import pandas as pd

url = "https://helensdiary.pixnet.net/blog/post/366606154?utm_source=PIXNET&utm_medium=ppage"
headers = {
    "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36"
}
response = requests.get(url, headers=headers)
soup = BeautifulSoup(response.content, 'html.parser')
articles = []

# æŠ“æ¨™é¡Œ
title_div = soup.find('div', class_='article')
title_tag = title_div.find('h2')
title = title_tag.text.strip()
print(title)

# ç™¼æ–‡æ—¥æœŸ
publish_li = soup.find('li', class_='publish')
if publish_li:
    text = publish_li.get_text(separator=' ', strip=True)
    print(text)
else:
    print("æ‰¾ä¸åˆ° publish çš„ li")

content = ""
content_div = soup.find('div', class_='article-content-inner')
p_tags = content_div.find_all('p')
content = "\n".join(p.text.strip() for p in p_tags)

articles.append([title,text,content])


df = pd.DataFrame(articles, columns=["title", "date","content"])

# æ”¾é€²DataFrame
df.info()
print(df)

df.to_csv("articles15.csv", encoding="utf-8")

"""# ç¬¬åå…­ç¯‡æ–‡ç« """

import requests
from bs4 import BeautifulSoup
import pandas as pd

url = "https://jack74327.pixnet.net/blog/post/71285884?utm_source=PIXNET&utm_medium=ppage"
headers = {
    "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36"
}
response = requests.get(url, headers=headers)
soup = BeautifulSoup(response.content, 'html.parser')
articles = []

# æŠ“æ¨™é¡Œ
title_div = soup.find('div', class_='article')
title_tag = title_div.find('h2')
title = title_tag.text.strip()
print(title)

# ç™¼æ–‡æ—¥æœŸ
publish_li = soup.find('li', class_='publish')
if publish_li:
    text = publish_li.get_text(separator=' ', strip=True)
    print(text)
else:
    print("æ‰¾ä¸åˆ° publish çš„ li")

content = ""
content_div = soup.find('div', class_='article-content-inner')
p_tags = content_div.find_all('p')
content = "\n".join(p.text.strip() for p in p_tags)

articles.append([title,text,content])


df = pd.DataFrame(articles, columns=["title", "date","content"])

# æ”¾é€²DataFrame
df.info()
print(df)

df.to_csv("articles16.csv", encoding="utf-8")

"""# ç¬¬åä¸ƒç¯‡æ–‡ç« """

# åŒ¯å…¥å¥—ä»¶
import requests
from bs4 import BeautifulSoup
import pandas as pd

# å®šç¾©çˆ¬èŸ²å‡½å¼
def scrape_pixnet_article(url: str, save_to_csv: bool = False, filename: str = "articles.csv") -> pd.DataFrame:
    headers = {
        "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36"
    }
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.content, 'html.parser')
    articles = []

    # æŠ“æ¨™é¡Œ
    title_div = soup.find('div', class_='article')
    title_tag = title_div.find('h2') if title_div else None
    title = title_tag.text.strip() if title_tag else "æ‰¾ä¸åˆ°æ¨™é¡Œ"

    # ç™¼æ–‡æ—¥æœŸ
    publish_li = soup.find('li', class_='publish')
    text = publish_li.get_text(separator=' ', strip=True) if publish_li else "æ‰¾ä¸åˆ°ç™¼æ–‡æ—¥æœŸ"

    # æŠ“å…§å®¹
    content_div = soup.find('div', class_='article-content-inner')
    if content_div:
        p_tags = content_div.find_all('p')
        content = "\n".join(p.text.strip() for p in p_tags)
    else:
        content = "æ‰¾ä¸åˆ°å…§å®¹å€å¡Š"

    articles.append([title, text, content])
    df = pd.DataFrame(articles, columns=["title", "date", "content"])

    if save_to_csv:
        df.to_csv(filename, encoding="utf-8", index=False)

    return df

# ä¸»ç¨‹å¼å…¥å£
if __name__ == "__main__":
    # è¼¸å…¥ä½ æƒ³çˆ¬çš„ PIXNET ç¶²å€
    url = "https://icela.pixnet.net/blog/post/49374036?utm_source=PIXNET&utm_medium=ppage"

    # å‘¼å«å‡½å¼ï¼Œçˆ¬å–æ–‡ç« ä¸¦å­˜æˆ CSV
    df = scrape_pixnet_article(url, save_to_csv=True, filename="articles17.csv")

    # å°å‡º DataFrame çµæœ
    print(df)

"""# ç¬¬åå…«ç¯‡æ–‡ç« """

# åŒ¯å…¥å¥—ä»¶
import requests
from bs4 import BeautifulSoup
import pandas as pd

# å®šç¾©çˆ¬èŸ²å‡½å¼
def scrape_pixnet_article(url: str, save_to_csv: bool = False, filename: str = "articles.csv") -> pd.DataFrame:
    headers = {
        "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36"
    }
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.content, 'html.parser')
    articles = []

    # æŠ“æ¨™é¡Œ
    title_div = soup.find('div', class_='article')
    title_tag = title_div.find('h2') if title_div else None
    title = title_tag.text.strip() if title_tag else "æ‰¾ä¸åˆ°æ¨™é¡Œ"

    # ç™¼æ–‡æ—¥æœŸ
    publish_li = soup.find('li', class_='publish')
    text = publish_li.get_text(separator=' ', strip=True) if publish_li else "æ‰¾ä¸åˆ°ç™¼æ–‡æ—¥æœŸ"

    # æŠ“å…§å®¹
    content_div = soup.find('div', class_='article-content-inner')
    if content_div:
        p_tags = content_div.find_all('p')
        content = "\n".join(p.text.strip() for p in p_tags)
    else:
        content = "æ‰¾ä¸åˆ°å…§å®¹å€å¡Š"

    articles.append([title, text, content])
    df = pd.DataFrame(articles, columns=["title", "date", "content"])

    if save_to_csv:
        df.to_csv(filename, encoding="utf-8", index=False)

    return df

# ä¸»ç¨‹å¼å…¥å£
if __name__ == "__main__":
    # è¼¸å…¥ä½ æƒ³çˆ¬çš„ PIXNET ç¶²å€
    url = "https://sujungyuo970926.pixnet.net/blog/post/345928714?utm_source=PIXNET&utm_medium=ppage"
    # å‘¼å«å‡½å¼ï¼Œçˆ¬å–æ–‡ç« ä¸¦å­˜æˆ CSV
    df = scrape_pixnet_article(url, save_to_csv=True, filename="articles18.csv")

    # å°å‡º DataFrame çµæœ
    print(df)

"""# å¯«æˆå‡½å¼"""

import requests
from bs4 import BeautifulSoup
import pandas as pd

# å®šç¾©çˆ¬èŸ²å‡½å¼
def scrape_pixnet_article(url: str) -> pd.DataFrame:
    headers = {
        "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36"
    }
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.content, 'html.parser')

    # æŠ“æ¨™é¡Œ
    title_div = soup.find('div', class_='article')
    title_tag = title_div.find('h2') if title_div else None
    title = title_tag.text.strip() if title_tag else "æ‰¾ä¸åˆ°æ¨™é¡Œ"

    # ç™¼æ–‡æ—¥æœŸ
    publish_li = soup.find('li', class_='publish')
    date = publish_li.get_text(separator=' ', strip=True) if publish_li else "æ‰¾ä¸åˆ°ç™¼æ–‡æ—¥æœŸ"

    # æŠ“å…§å®¹
    content_div = soup.find('div', class_='article-content-inner')
    if content_div:
        p_tags = content_div.find_all('p')
        content = "\n".join(p.text.strip() for p in p_tags)
    else:
        content = "æ‰¾ä¸åˆ°å…§å®¹å€å¡Š"

    return pd.DataFrame([[title, date, content]], columns=["title", "date", "content"])

# ä¸»ç¨‹å¼ï¼šå¤šç¶²å€ã€å¤šCSVæª”æ¡ˆ
if __name__ == "__main__":
    urls = [
        "https://evshhips.pixnet.net/blog/post/357477369?utm_source=PIXNET&utm_medium=ppage",
        "https://chant198983.pixnet.net/blog/post/234053553?utm_source=PIXNET&utm_medium=ppage",
        "https://chant198983.pixnet.net/blog/post/234049989?utm_source=PIXNET&utm_medium=ppage",
        # ä½ å¯ä»¥ç¹¼çºŒåŠ å…¥æ›´å¤šç¶²å€
    ]

    start_index = 19  # å¾ articles18.csv é–‹å§‹å‘½å

    for i, url in enumerate(urls, start=start_index):
        try:
            df = scrape_pixnet_article(url)
            filename = f"articles{i}.csv"
            df.to_csv(filename, encoding="utf-8", index=False)
            print(f"âœ… å„²å­˜æˆåŠŸï¼š{filename}")
        except Exception as e:
            print(f"âŒ éŒ¯èª¤ï¼š{url} -> {e}")

import requests
from bs4 import BeautifulSoup
import pandas as pd

# å®šç¾©çˆ¬èŸ²å‡½å¼
def scrape_pixnet_article(url: str) -> pd.DataFrame:
    headers = {
        "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36"
    }
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.content, 'html.parser')

    # æŠ“æ¨™é¡Œ
    title_div = soup.find('div', class_='article')
    title_tag = title_div.find('h2') if title_div else None
    title = title_tag.text.strip() if title_tag else "æ‰¾ä¸åˆ°æ¨™é¡Œ"

    # ç™¼æ–‡æ—¥æœŸ
    publish_li = soup.find('li', class_='publish')
    date = publish_li.get_text(separator=' ', strip=True) if publish_li else "æ‰¾ä¸åˆ°ç™¼æ–‡æ—¥æœŸ"

    # æŠ“å…§å®¹
    content_div = soup.find('div', class_='article-content-inner')
    if content_div:
        p_tags = content_div.find_all('p')
        content = "\n".join(p.text.strip() for p in p_tags)
    else:
        content = "æ‰¾ä¸åˆ°å…§å®¹å€å¡Š"

    return pd.DataFrame([[title, date, content]], columns=["title", "date", "content"])

# ä¸»ç¨‹å¼ï¼šå¤šç¶²å€ã€å¤šCSVæª”æ¡ˆ
if __name__ == "__main__":
    urls = [
        "https://kk961208.pixnet.net/blog/post/576743152?utm_source=PIXNET&utm_medium=ppage",
        "https://spirit0926.pixnet.net/blog/post/50852228?utm_source=PIXNET&utm_medium=ppage",
        "https://campingshark86.pixnet.net/blog/post/155059279?utm_source=PIXNET&utm_medium=ppage",
        "https://sujungyuo970926.pixnet.net/blog/post/345498550?utm_source=PIXNET&utm_medium=ppage"
        # ä½ å¯ä»¥ç¹¼çºŒåŠ å…¥æ›´å¤šç¶²å€
    ]

    start_index = 22  # å¾ articles18.csv é–‹å§‹å‘½å

    for i, url in enumerate(urls, start=start_index):
        try:
            df = scrape_pixnet_article(url)
            filename = f"articles{i}.csv"
            df.to_csv(filename, encoding="utf-8", index=False)
            print(f"âœ… å„²å­˜æˆåŠŸï¼š{filename}")
        except Exception as e:
            print(f"âŒ éŒ¯èª¤ï¼š{url} -> {e}")

import requests
from bs4 import BeautifulSoup
import pandas as pd

# å®šç¾©çˆ¬èŸ²å‡½å¼
def scrape_pixnet_article(url: str) -> pd.DataFrame:
    headers = {
        "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36"
    }
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.content, 'html.parser')

    # æŠ“æ¨™é¡Œ
    title_div = soup.find('div', class_='article')
    title_tag = title_div.find('h2') if title_div else None
    title = title_tag.text.strip() if title_tag else "æ‰¾ä¸åˆ°æ¨™é¡Œ"

    # ç™¼æ–‡æ—¥æœŸ
    publish_li = soup.find('li', class_='publish')
    date = publish_li.get_text(separator=' ', strip=True) if publish_li else "æ‰¾ä¸åˆ°ç™¼æ–‡æ—¥æœŸ"

    # æŠ“å…§å®¹
    content_div = soup.find('div', class_='article-content-inner')
    if content_div:
        p_tags = content_div.find_all('p')
        content = "\n".join(p.text.strip() for p in p_tags)
    else:
        content = "æ‰¾ä¸åˆ°å…§å®¹å€å¡Š"

    return pd.DataFrame([[title, date, content]], columns=["title", "date", "content"])

# ä¸»ç¨‹å¼ï¼šå¤šç¶²å€ã€å¤šCSVæª”æ¡ˆ
if __name__ == "__main__":
    urls = [
     "https://pimama.pixnet.net/blog/post/152847460?utm_source=PIXNET&utm_medium=ppage",
     "https://wtw332589.pixnet.net/blog/post/345111304?utm_source=PIXNET&utm_medium=ppage",
     "https://fan202189.pixnet.net/blog/post/151067122?utm_source=PIXNET&utm_medium=ppage",
     "https://umechen.pixnet.net/blog/post/34019597?utm_source=PIXNET&utm_medium=ppage",
     "https://wtw332589.pixnet.net/blog/post/344783791?utm_source=PIXNET&utm_medium=ppage",
     "https://monkeybowbi.pixnet.net/blog/post/576113508?utm_source=PIXNET&utm_medium=ppage",
     "https://vivi0010.pixnet.net/blog/post/233723769?utm_source=PIXNET&utm_medium=ppage",
     "https://hara5415.pixnet.net/blog/post/355623421?utm_source=PIXNET&utm_medium=ppage",
     "https://pigpig0412.pixnet.net/blog/post/233020036?utm_source=PIXNET&utm_medium=ppage",
     "https://q3621533.pixnet.net/blog/post/569110372?utm_source=PIXNET&utm_medium=ppage",
     "https://sammi0224.pixnet.net/blog/post/121968776?utm_source=PIXNET&utm_medium=ppage",
     "https://win960927.pixnet.net/blog/post/576034040?utm_source=PIXNET&utm_medium=ppage",
     "https://f405510017.pixnet.net/blog/post/341933520?utm_source=PIXNET&utm_medium=ppage",
     "https://sammima.pixnet.net/blog/post/121713461?utm_source=PIXNET&utm_medium=ppage",
     "https://hochusay.pixnet.net/blog/post/102176588?utm_source=PIXNET&utm_medium=ppage",
     "https://tom20030208.pixnet.net/blog/post/339989645?utm_source=PIXNET&utm_medium=ppage",
     "https://ctinas604.pixnet.net/blog/post/340289715?utm_source=PIXNET&utm_medium=ppage",
     "https://unatsai525.pixnet.net/blog/post/571181516?utm_source=PIXNET&utm_medium=ppage",
     "https://anity0404.pixnet.net/blog/post/232514797?utm_source=PIXNET&utm_medium=ppage",
     "https://hara5415.pixnet.net/blog/post/354633634?utm_source=PIXNET&utm_medium=ppage",
     "https://joycerb.pixnet.net/blog/post/336962477?utm_source=PIXNET&utm_medium=ppage",
     "https://pinke0324.pixnet.net/blog/post/565055590?utm_source=PIXNET&utm_medium=ppage",
     "https://r510517.pixnet.net/blog/post/335249041?utm_source=PIXNET&utm_medium=ppage",
     "https://sic27ap.pixnet.net/blog/post/50067124?utm_source=PIXNET&utm_medium=ppage",
     "https://pbecky77.pixnet.net/blog/post/355242367?utm_source=PIXNET&utm_medium=ppage",
     "https://liankaogo.pixnet.net/blog/post/55329370?utm_source=PIXNET&utm_medium=ppage",
     "https://q3621533.pixnet.net/blog/post/565065358?utm_source=PIXNET&utm_medium=ppage"

      # ä½ å¯ä»¥ç¹¼çºŒåŠ å…¥æ›´å¤šç¶²å€
    ]

    start_index = 26  # å¾ articles18.csv é–‹å§‹å‘½å

    for i, url in enumerate(urls, start=start_index):
        try:
            df = scrape_pixnet_article(url)
            filename = f"articles{i}.csv"
            df.to_csv(filename, encoding="utf-8", index=False)
            print(f"âœ… å„²å­˜æˆåŠŸï¼š{filename}")
        except Exception as e:
            print(f"âŒ éŒ¯èª¤ï¼š{url} -> {e}")

import requests
from bs4 import BeautifulSoup
import pandas as pd

# å®šç¾©çˆ¬èŸ²å‡½å¼
def scrape_pixnet_article(url: str) -> pd.DataFrame:
    headers = {
        "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36"
    }
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.content, 'html.parser')

    # æŠ“æ¨™é¡Œ
    title_div = soup.find('div', class_='article')
    title_tag = title_div.find('h2') if title_div else None
    title = title_tag.text.strip() if title_tag else "æ‰¾ä¸åˆ°æ¨™é¡Œ"

    # ç™¼æ–‡æ—¥æœŸ
    publish_li = soup.find('li', class_='publish')
    date = publish_li.get_text(separator=' ', strip=True) if publish_li else "æ‰¾ä¸åˆ°ç™¼æ–‡æ—¥æœŸ"

    # æŠ“å…§å®¹
    content_div = soup.find('div', class_='article-content-inner')
    if content_div:
        p_tags = content_div.find_all('p')
        content = "\n".join(p.text.strip() for p in p_tags)
    else:
        content = "æ‰¾ä¸åˆ°å…§å®¹å€å¡Š"

    return pd.DataFrame([[title, date, content]], columns=["title", "date", "content"])

# ä¸»ç¨‹å¼ï¼šå¤šç¶²å€ã€å¤šCSVæª”æ¡ˆ
if __name__ == "__main__":
    urls = [
     "https://jarvi2s.pixnet.net/blog/post/61074877?utm_source=PIXNET&utm_medium=ppage",
     "https://pei95416.pixnet.net/blog/post/362602541?utm_source=PIXNET&utm_medium=ppage",
     "https://wakabayashi.pixnet.net/blog/post/63922354?utm_source=PIXNET&utm_medium=ppage",
     "https://jush.pixnet.net/blog/post/225975101?utm_source=PIXNET&utm_medium=ppage",
     "https://ywayway.pixnet.net/blog/post/121539864?utm_source=PIXNET&utm_medium=ppage",
     "https://openchiang1113.pixnet.net/blog/post/121400824?utm_source=PIXNET&utm_medium=ppage",
     "https://openchiang1113.pixnet.net/blog/post/121339288?utm_source=PIXNET&utm_medium=ppage",
     "https://kikimilk2.pixnet.net/blog/post/336223181?utm_source=PIXNET&utm_medium=ppage",
     "https://kikimilk2.pixnet.net/blog/post/336605697?utm_source=PIXNET&utm_medium=ppage",
     "https://chiafei123.pixnet.net/blog/post/362798901?utm_source=PIXNET&utm_medium=ppage",
     "https://ctinas604.pixnet.net/blog/post/339463815?utm_source=PIXNET&utm_medium=ppage",
     "https://mikaaaaa0907.pixnet.net/blog/post/226150231?utm_source=PIXNET&utm_medium=ppage",
     "https://hara5415.pixnet.net/blog/post/354059587?utm_source=PIXNET&utm_medium=ppage",
     "https://meiworld.pixnet.net/blog/post/53612593?utm_source=PIXNET&utm_medium=ppage",
     "https://pbecky77.pixnet.net/blog/post/353872414?utm_source=PIXNET&utm_medium=ppage",
     "https://jnr283.pixnet.net/blog/post/225325315?utm_source=PIXNET&utm_medium=ppage"

      # ä½ å¯ä»¥ç¹¼çºŒåŠ å…¥æ›´å¤šç¶²å€
    ]

    start_index = 53  # å¾ articles18.csv é–‹å§‹å‘½å

    for i, url in enumerate(urls, start=start_index):
        try:
            df = scrape_pixnet_article(url)
            filename = f"articles{i}.csv"
            df.to_csv(filename, encoding="utf-8", index=False)
            print(f"âœ… å„²å­˜æˆåŠŸï¼š{filename}")
        except Exception as e:
            print(f"âŒ éŒ¯èª¤ï¼š{url} -> {e}")

import requests
from bs4 import BeautifulSoup
import pandas as pd

# å®šç¾©çˆ¬èŸ²å‡½å¼
def scrape_pixnet_article(url: str) -> pd.DataFrame:
    headers = {
        "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36"
    }
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.content, 'html.parser')

    # æŠ“æ¨™é¡Œ
    title_div = soup.find('div', class_='article')
    title_tag = title_div.find('h2') if title_div else None
    title = title_tag.text.strip() if title_tag else "æ‰¾ä¸åˆ°æ¨™é¡Œ"

    # ç™¼æ–‡æ—¥æœŸ
    publish_li = soup.find('li', class_='publish')
    date = publish_li.get_text(separator=' ', strip=True) if publish_li else "æ‰¾ä¸åˆ°ç™¼æ–‡æ—¥æœŸ"

    # æŠ“å…§å®¹
    content_div = soup.find('div', class_='article-content-inner')
    if content_div:
        p_tags = content_div.find_all('p')
        content = "\n".join(p.text.strip() for p in p_tags)
    else:
        content = "æ‰¾ä¸åˆ°å…§å®¹å€å¡Š"

    return pd.DataFrame([[title, date, content]], columns=["title", "date", "content"])

# ä¸»ç¨‹å¼ï¼šå¤šç¶²å€ã€å¤šCSVæª”æ¡ˆ
if __name__ == "__main__":
    urls = [
     "https://veneaus.pixnet.net/blog/post/329941240?utm_source=PIXNET&utm_medium=ppage",
     "https://gin762001.pixnet.net/blog/post/42156274?utm_source=PIXNET&utm_medium=ppage",
     "https://gin762001.pixnet.net/blog/post/54491344?utm_source=PIXNET&utm_medium=ppage",
     "https://chunella726.pixnet.net/blog/post/353589366?utm_source=PIXNET&utm_medium=ppage",
     "https://a9x22t3eh.pixnet.net/blog/post/354367369?utm_source=PIXNET&utm_medium=ppage",
     "https://kikimilk2.pixnet.net/blog/post/332886601?utm_source=PIXNET&utm_medium=ppage",
     "https://smilefishfish.pixnet.net/blog/post/404953714?utm_source=PIXNET&utm_medium=ppage",
     "https://wowitspeggy.pixnet.net/blog/post/556841071?utm_source=PIXNET&utm_medium=ppage",
     "https://tadli.pixnet.net/blog/post/229930776?utm_source=PIXNET&utm_medium=ppage",
     "https://tadli.pixnet.net/blog/post/229706105?utm_source=PIXNET&utm_medium=ppage",
     "https://jmy7296.pixnet.net/blog/post/120799565?utm_source=PIXNET&utm_medium=ppage",
     "https://ni70043.pixnet.net/blog/post/351701242?utm_source=PIXNET&utm_medium=ppage",
     "https://weibaby0109.pixnet.net/blog/post/68693681?utm_source=PIXNET&utm_medium=ppage",
     "https://sunny7028.pixnet.net/blog/post/359403376?utm_source=PIXNET&utm_medium=ppage",
     "https://even615.pixnet.net/blog/post/120011586?utm_source=PIXNET&utm_medium=ppage",
     "https://e583i.pixnet.net/blog/post/67773750?utm_source=PIXNET&utm_medium=ppage",
     "https://e583i.pixnet.net/blog/post/67434423?utm_source=PIXNET&utm_medium=ppage",
     "https://bbpeng2.pixnet.net/blog/post/228740768?utm_source=PIXNET&utm_medium=ppage",
     "https://yingjun1109.pixnet.net/blog/post/35018896?utm_source=PIXNET&utm_medium=ppage",
     "https://lilstep.pixnet.net/blog/post/173117754?utm_source=PIXNET&utm_medium=ppage"


      # ä½ å¯ä»¥ç¹¼çºŒåŠ å…¥æ›´å¤šç¶²å€
    ]

    start_index = 69  # å¾ articles18.csv é–‹å§‹å‘½å

    for i, url in enumerate(urls, start=start_index):
        try:
            df = scrape_pixnet_article(url)
            filename = f"articles{i}.csv"
            df.to_csv(filename, encoding="utf-8", index=False)
            print(f"âœ… å„²å­˜æˆåŠŸï¼š{filename}")
        except Exception as e:
            print(f"âŒ éŒ¯èª¤ï¼š{url} -> {e}")

import requests
from bs4 import BeautifulSoup
import pandas as pd

# å®šç¾©çˆ¬èŸ²å‡½å¼
def scrape_pixnet_article(url: str) -> pd.DataFrame:
    headers = {
        "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36"
    }
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.content, 'html.parser')

    # æŠ“æ¨™é¡Œ
    title_div = soup.find('div', class_='article')
    title_tag = title_div.find('h2') if title_div else None
    title = title_tag.text.strip() if title_tag else "æ‰¾ä¸åˆ°æ¨™é¡Œ"

    # ç™¼æ–‡æ—¥æœŸ
    publish_li = soup.find('li', class_='publish')
    date = publish_li.get_text(separator=' ', strip=True) if publish_li else "æ‰¾ä¸åˆ°ç™¼æ–‡æ—¥æœŸ"

    # æŠ“å…§å®¹
    content_div = soup.find('div', class_='article-content-inner')
    if content_div:
        p_tags = content_div.find_all('p')
        content = "\n".join(p.text.strip() for p in p_tags)
    else:
        content = "æ‰¾ä¸åˆ°å…§å®¹å€å¡Š"

    return pd.DataFrame([[title, date, content]], columns=["title", "date", "content"])

# ä¸»ç¨‹å¼ï¼šå¤šç¶²å€ã€å¤šCSVæª”æ¡ˆ
if __name__ == "__main__":
    urls = [
     "https://gn10202000.pixnet.net/blog/post/466533818?utm_source=PIXNET&utm_medium=ppage",
     "https://realobasan.pixnet.net/blog/post/264050196?utm_source=PIXNET&utm_medium=ppage",
     "https://bbpeng2.pixnet.net/blog/post/216870913?utm_source=PIXNET&utm_medium=ppage",
     "https://bbpeng2.pixnet.net/blog/post/222291115?utm_source=PIXNET&utm_medium=ppage",
     "https://caramelmm.pixnet.net/blog/post/301192867?utm_source=PIXNET&utm_medium=ppage#google_vignette",
     "https://hohopig.pixnet.net/blog/post/450205724?utm_source=PIXNET&utm_medium=ppage",
     "https://hohopig.pixnet.net/blog/post/447248663?utm_source=PIXNET&utm_medium=ppage",
     "https://bbpeng2.pixnet.net/blog/post/221060027?utm_source=PIXNET&utm_medium=ppage",
     "https://yumi3255.pixnet.net/blog/post/288964081?utm_source=PIXNET&utm_medium=ppage",
     "https://sammi0224.pixnet.net/blog/post/112621492?utm_source=PIXNET&utm_medium=ppage"


      # ä½ å¯ä»¥ç¹¼çºŒåŠ å…¥æ›´å¤šç¶²å€
    ]

    start_index = 89  # å¾ articles18.csv é–‹å§‹å‘½å

    for i, url in enumerate(urls, start=start_index):
        try:
            df = scrape_pixnet_article(url)
            filename = f"articles{i}.csv"
            df.to_csv(filename, encoding="utf-8", index=False)
            print(f"âœ… å„²å­˜æˆåŠŸï¼š{filename}")
        except Exception as e:
            print(f"âŒ éŒ¯èª¤ï¼š{url} -> {e}")

import requests
from bs4 import BeautifulSoup
import pandas as pd

# å®šç¾©çˆ¬èŸ²å‡½å¼
def scrape_pixnet_article(url: str) -> pd.DataFrame:
    headers = {
        "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36"
    }
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.content, 'html.parser')

    # æŠ“æ¨™é¡Œ
    title_div = soup.find('div', class_='article')
    title_tag = title_div.find('h2') if title_div else None
    title = title_tag.text.strip() if title_tag else "æ‰¾ä¸åˆ°æ¨™é¡Œ"

    # ç™¼æ–‡æ—¥æœŸ
    publish_li = soup.find('li', class_='publish')
    date = publish_li.get_text(separator=' ', strip=True) if publish_li else "æ‰¾ä¸åˆ°ç™¼æ–‡æ—¥æœŸ"

    # æŠ“å…§å®¹
    content_div = soup.find('div', class_='article-content-inner')
    if content_div:
        p_tags = content_div.find_all('p')
        content = "\n".join(p.text.strip() for p in p_tags)
    else:
        content = "æ‰¾ä¸åˆ°å…§å®¹å€å¡Š"

    return pd.DataFrame([[title, date, content]], columns=["title", "date", "content"])

# ä¸»ç¨‹å¼ï¼šå¤šç¶²å€ã€å¤šCSVæª”æ¡ˆ
if __name__ == "__main__":
    urls = [
     "https://yourich.pixnet.net/blog/post/358307730?utm_source=PIXNET&utm_medium=ppage",
     "https://unatsai525.pixnet.net/blog/post/572311028",
     "https://paicj.pixnet.net/blog/post/407211841?utm_source=PIXNET&utm_medium=ppage"




      # ä½ å¯ä»¥ç¹¼çºŒåŠ å…¥æ›´å¤šç¶²å€
    ]

    start_index = 99  # å¾ articles18.csv é–‹å§‹å‘½å

    for i, url in enumerate(urls, start=start_index):
        try:
            df = scrape_pixnet_article(url)
            filename = f"articles{i}.csv"
            df.to_csv(filename, encoding="utf-8", index=False)
            print(f"âœ… å„²å­˜æˆåŠŸï¼š{filename}")
        except Exception as e:
            print(f"âŒ éŒ¯èª¤ï¼š{url} -> {e}")

import requests
from bs4 import BeautifulSoup
import pandas as pd

# å®šç¾©çˆ¬èŸ²å‡½å¼
def scrape_pixnet_article(url: str) -> pd.DataFrame:
    headers = {
        "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36"
    }
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.content, 'html.parser')

    # æŠ“æ¨™é¡Œ
    title_div = soup.find('div', class_='article')
    title_tag = title_div.find('h2') if title_div else None
    title = title_tag.text.strip() if title_tag else "æ‰¾ä¸åˆ°æ¨™é¡Œ"

    # ç™¼æ–‡æ—¥æœŸ
    publish_li = soup.find('li', class_='publish')
    date = publish_li.get_text(separator=' ', strip=True) if publish_li else "æ‰¾ä¸åˆ°ç™¼æ–‡æ—¥æœŸ"

    # æŠ“å…§å®¹
    content_div = soup.find('div', class_='article-content-inner')
    if content_div:
        p_tags = content_div.find_all('p')
        content = "\n".join(p.text.strip() for p in p_tags)
    else:
        content = "æ‰¾ä¸åˆ°å…§å®¹å€å¡Š"

    return pd.DataFrame([[title, date, content]], columns=["title", "date", "content"])

# ä¸»ç¨‹å¼ï¼šå¤šç¶²å€ã€å¤šCSVæª”æ¡ˆ
if __name__ == "__main__":
    urls = [
    "https://sunboy0722.pixnet.net/blog/post/358293774?utm_source=PIXNET&utm_medium=ppage",
    "https://paicj.pixnet.net/blog/post/407201177?utm_source=PIXNET&utm_medium=ppage",
    "https://sunboy0722.pixnet.net/blog/post/357915222?utm_source=PIXNET&utm_medium=ppage",
    "https://liankaogo.pixnet.net/blog/post/167720920?utm_source=PIXNET&utm_medium=ppage",
    "https://aaa22034381.pixnet.net/blog/post/165951676?utm_source=PIXNET&utm_medium=ppage",
    "https://aaa22034381.pixnet.net/blog/post/166209856?utm_source=PIXNET&utm_medium=ppage",
    "https://huakuanbin.pixnet.net/blog/post/366697744?utm_source=PIXNET&utm_medium=ppage",
    "https://lilyliketoeat594.pixnet.net/blog/post/162573973?utm_source=PIXNET&utm_medium=ppage",
    "https://huakuanbin.pixnet.net/blog/post/366617770?utm_source=PIXNET&utm_medium=ppage",
    "https://showwen1011.pixnet.net/blog/post/346099969?utm_source=PIXNET&utm_medium=ppage",
    "https://abby0318.pixnet.net/blog/post/576910492?utm_source=PIXNET&utm_medium=ppage",
    "https://evshhips.pixnet.net/blog/post/357307161?utm_source=PIXNET&utm_medium=ppage",
    "https://cndjourney.pixnet.net/blog/post/341828409?utm_source=PIXNET&utm_medium=ppage",
    "https://sunboy0722.pixnet.net/blog/post/357345927?utm_source=PIXNET&utm_medium=ppage",
    "https://tadli.pixnet.net/blog/post/234440256?utm_source=PIXNET&utm_medium=ppage",
    "https://sunboy0722.pixnet.net/blog/post/358004763?utm_source=PIXNET&utm_medium=ppage",
    "https://sunboy0722.pixnet.net/blog/post/357959940?utm_source=PIXNET&utm_medium=ppage",
    "https://paicj.pixnet.net/blog/post/407108917?utm_source=PIXNET&utm_medium=ppage",
    "https://white91.pixnet.net/blog/post/169470331?utm_source=PIXNET&utm_medium=ppage",
    "https://lilyliketoeat594.pixnet.net/blog/post/169269448?utm_source=PIXNET&utm_medium=ppage",
    "https://chant198983.pixnet.net/blog/post/234551052?utm_source=PIXNET&utm_medium=ppage",
    "https://ysm1121321.pixnet.net/blog/post/366855304?utm_source=PIXNET&utm_medium=ppage"



      # ä½ å¯ä»¥ç¹¼çºŒåŠ å…¥æ›´å¤šç¶²å€
    ]

    start_index = 102  # å¾ articles18.csv é–‹å§‹å‘½å

    for i, url in enumerate(urls, start=start_index):
        try:
            df = scrape_pixnet_article(url)
            filename = f"articles{i}.csv"
            df.to_csv(filename, encoding="utf-8", index=False)
            print(f"âœ… å„²å­˜æˆåŠŸï¼š{filename}")
        except Exception as e:
            print(f"âŒ éŒ¯èª¤ï¼š{url} -> {e}")

import requests
from bs4 import BeautifulSoup
import pandas as pd

# å®šç¾©çˆ¬èŸ²å‡½å¼
def scrape_pixnet_article(url: str) -> pd.DataFrame:
    headers = {
        "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36"
    }
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.content, 'html.parser')

    # æŠ“æ¨™é¡Œ
    title_div = soup.find('div', class_='article')
    title_tag = title_div.find('h2') if title_div else None
    title = title_tag.text.strip() if title_tag else "æ‰¾ä¸åˆ°æ¨™é¡Œ"

    # ç™¼æ–‡æ—¥æœŸ
    publish_li = soup.find('li', class_='publish')
    date = publish_li.get_text(separator=' ', strip=True) if publish_li else "æ‰¾ä¸åˆ°ç™¼æ–‡æ—¥æœŸ"

    # æŠ“å…§å®¹
    content_div = soup.find('div', class_='article-content-inner')
    if content_div:
        p_tags = content_div.find_all('p')
        content = "\n".join(p.text.strip() for p in p_tags)
    else:
        content = "æ‰¾ä¸åˆ°å…§å®¹å€å¡Š"

    return pd.DataFrame([[title, date, content]], columns=["title", "date", "content"])

# ä¸»ç¨‹å¼ï¼šå¤šç¶²å€ã€å¤šCSVæª”æ¡ˆ
if __name__ == "__main__":
    urls = [
   "https://pimama.pixnet.net/blog/post/152852305?utm_source=PIXNET&utm_medium=ppage",
   "https://hsinchueric.pixnet.net/blog/post/149364976?utm_source=PIXNET&utm_medium=ppage",
   "https://lovebaby31.pixnet.net/blog/post/222522742?utm_source=PIXNET&utm_medium=ppage",
   "https://camptrip.pixnet.net/blog/post/132346114?utm_source=PIXNET&utm_medium=ppage",
   "https://hsinchueric.pixnet.net/blog/post/140530129?utm_source=PIXNET&utm_medium=ppage",
   "https://ctinas604.pixnet.net/blog/post/343974406?utm_source=PIXNET&utm_medium=ppage",
   "https://camellia8283.pixnet.net/blog/post/100594817?utm_source=PIXNET&utm_medium=ppage",
   "https://liankaogo.pixnet.net/blog/post/100982982?utm_source=PIXNET&utm_medium=ppage",
   "https://yunjie059.pixnet.net/blog/post/104826901?utm_source=PIXNET&utm_medium=ppage",
   "https://aaa22034381.pixnet.net/blog/post/87537181?utm_source=PIXNET&utm_medium=ppage",
   "https://evonne1205.pixnet.net/blog/post/101978516?utm_source=PIXNET&utm_medium=ppage",
   "https://katrina1207.pixnet.net/blog/post/122059375?utm_source=PIXNET&utm_medium=ppage",
   "https://kimandmonica.pixnet.net/blog/post/92928679?utm_source=PIXNET&utm_medium=ppage",
   "https://may0708.pixnet.net/blog/post/569938760?utm_source=PIXNET&utm_medium=ppage",
   "https://twoncinpa2.pixnet.net/blog/post/364316233?utm_source=PIXNET&utm_medium=ppage",
   "https://yunny0421.pixnet.net/blog/post/337607941?utm_source=PIXNET&utm_medium=ppage",
   "https://annatree2014.pixnet.net/blog/post/356849149?utm_source=PIXNET&utm_medium=ppage",
   "https://cndjourney.pixnet.net/blog/post/338423425?utm_source=PIXNET&utm_medium=ppage",
   "https://kk961208.pixnet.net/blog/post/567606048?utm_source=PIXNET&utm_medium=ppage",
   "https://penny800302.pixnet.net/blog/post/340334238?utm_source=PIXNET&utm_medium=ppage"



      # ä½ å¯ä»¥ç¹¼çºŒåŠ å…¥æ›´å¤šç¶²å€
    ]

    start_index = 124  # å¾ articles18.csv é–‹å§‹å‘½å

    for i, url in enumerate(urls, start=start_index):
        try:
            df = scrape_pixnet_article(url)
            filename = f"articles{i}.csv"
            df.to_csv(filename, encoding="utf-8", index=False)
            print(f"âœ… å„²å­˜æˆåŠŸï¼š{filename}")
        except Exception as e:
            print(f"âŒ éŒ¯èª¤ï¼š{url} -> {e}")

import requests
from bs4 import BeautifulSoup
import pandas as pd

# å®šç¾©çˆ¬èŸ²å‡½å¼
def scrape_pixnet_article(url: str) -> pd.DataFrame:
    headers = {
        "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36"
    }
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.content, 'html.parser')

    # æŠ“æ¨™é¡Œ
    title_div = soup.find('div', class_='article')
    title_tag = title_div.find('h2') if title_div else None
    title = title_tag.text.strip() if title_tag else "æ‰¾ä¸åˆ°æ¨™é¡Œ"

    # ç™¼æ–‡æ—¥æœŸ
    publish_li = soup.find('li', class_='publish')
    date = publish_li.get_text(separator=' ', strip=True) if publish_li else "æ‰¾ä¸åˆ°ç™¼æ–‡æ—¥æœŸ"

    # æŠ“å…§å®¹
    content_div = soup.find('div', class_='article-content-inner')
    if content_div:
        p_tags = content_div.find_all('p')
        content = "\n".join(p.text.strip() for p in p_tags)
    else:
        content = "æ‰¾ä¸åˆ°å…§å®¹å€å¡Š"

    return pd.DataFrame([[title, date, content]], columns=["title", "date", "content"])

# ä¸»ç¨‹å¼ï¼šå¤šç¶²å€ã€å¤šCSVæª”æ¡ˆ
if __name__ == "__main__":
    urls = [
   "https://pandafish2018.pixnet.net/blog/post/343879596?utm_source=PIXNET&utm_medium=ppage",
   "https://ctinas604.pixnet.net/blog/post/340574949?utm_source=PIXNET&utm_medium=ppage",
   "https://ctinas604.pixnet.net/blog/post/341799597?utm_source=PIXNET&utm_medium=ppage",
   "https://ctinas604.pixnet.net/blog/post/339959685?utm_source=PIXNET&utm_medium=ppage",
   "https://cc710510.pixnet.net/blog/post/572647492?utm_source=PIXNET&utm_medium=ppage",
   "https://unatsai525.pixnet.net/blog/post/572311028?utm_source=PIXNET&utm_medium=ppage",
   "https://unatsai525.pixnet.net/blog/post/571998640?utm_source=PIXNET&utm_medium=ppage",
   "https://even615.pixnet.net/blog/post/121435900?utm_source=PIXNET&utm_medium=ppage",
   "https://yanyang.pixnet.net/blog/post/232209004?utm_source=PIXNET&utm_medium=ppage",
   "https://yunny0421.pixnet.net/blog/post/336229645?utm_source=PIXNET&utm_medium=ppage",
   "https://momocowang.pixnet.net/blog/post/566759028?utm_source=PIXNET&utm_medium=ppage",
   "https://digumom.pixnet.net/blog/post/64852633?utm_source=PIXNET&utm_medium=ppage",
   "https://sic27ap.pixnet.net/blog/post/49957276?utm_source=PIXNET&utm_medium=ppage#google_vignette",
   "https://whitney03050305.pixnet.net/blog/post/354916648?utm_source=PIXNET&utm_medium=ppage",
   "https://joyiner1988.pixnet.net/blog/post/73267696?utm_source=PIXNET&utm_medium=ppage",
   "https://chichulife.pixnet.net/blog/post/225978365?utm_source=PIXNET&utm_medium=ppage",
   "https://wakabayashi.pixnet.net/blog/post/64320496?utm_source=PIXNET&utm_medium=ppage",
   "https://aaa22034381.pixnet.net/blog/post/62396833?utm_source=PIXNET&utm_medium=ppage",
   "https://minkuo.pixnet.net/blog/post/121377890?utm_source=PIXNET&utm_medium=ppage",
   "https://cy4103134.pixnet.net/blog/post/363796426?utm_source=PIXNET&utm_medium=ppage"



      # ä½ å¯ä»¥ç¹¼çºŒåŠ å…¥æ›´å¤šç¶²å€
    ]

    start_index = 144  # å¾ articles18.csv é–‹å§‹å‘½å

    for i, url in enumerate(urls, start=start_index):
        try:
            df = scrape_pixnet_article(url)
            filename = f"articles{i}.csv"
            df.to_csv(filename, encoding="utf-8", index=False)
            print(f"âœ… å„²å­˜æˆåŠŸï¼š{filename}")
        except Exception as e:
            print(f"âŒ éŒ¯èª¤ï¼š{url} -> {e}")

import requests
from bs4 import BeautifulSoup
import pandas as pd

# å®šç¾©çˆ¬èŸ²å‡½å¼
def scrape_pixnet_article(url: str) -> pd.DataFrame:
    headers = {
        "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36"
    }
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.content, 'html.parser')

    # æŠ“æ¨™é¡Œ
    title_div = soup.find('div', class_='article')
    title_tag = title_div.find('h2') if title_div else None
    title = title_tag.text.strip() if title_tag else "æ‰¾ä¸åˆ°æ¨™é¡Œ"

    # ç™¼æ–‡æ—¥æœŸ
    publish_li = soup.find('li', class_='publish')
    date = publish_li.get_text(separator=' ', strip=True) if publish_li else "æ‰¾ä¸åˆ°ç™¼æ–‡æ—¥æœŸ"

    # æŠ“å…§å®¹
    content_div = soup.find('div', class_='article-content-inner')
    if content_div:
        p_tags = content_div.find_all('p')
        content = "\n".join(p.text.strip() for p in p_tags)
    else:
        content = "æ‰¾ä¸åˆ°å…§å®¹å€å¡Š"

    return pd.DataFrame([[title, date, content]], columns=["title", "date", "content"])

# ä¸»ç¨‹å¼ï¼šå¤šç¶²å€ã€å¤šCSVæª”æ¡ˆ
if __name__ == "__main__":
    urls = [
   "https://hayleyyu3306.pixnet.net/blog/post/221231493?utm_source=PIXNET&utm_medium=ppage",
   "https://hayleyyu3306.pixnet.net/blog/post/221366499?utm_source=PIXNET&utm_medium=ppage",
   "https://hayleyyu3306.pixnet.net/blog/post/221602176?utm_source=PIXNET&utm_medium=ppage",
   "https://kikimilk2.pixnet.net/blog/post/335397813?utm_source=PIXNET&utm_medium=ppage",
   "https://hero789456.pixnet.net/blog/post/221459301?utm_source=PIXNET&utm_medium=ppage",
   "https://even615.pixnet.net/blog/post/120998516?utm_source=PIXNET&utm_medium=ppage",
   "https://katetravel520.pixnet.net/blog/post/225192403?utm_source=PIXNET&utm_medium=ppage",
   "https://meiworld.pixnet.net/blog/post/51424978?utm_source=PIXNET&utm_medium=ppage",
   "https://aliceeeee.pixnet.net/blog/post/35187193?utm_source=PIXNET&utm_medium=ppage",
   "https://momocowang.pixnet.net/blog/post/557048950?utm_source=PIXNET&utm_medium=ppage",
   "https://ingrid0604.pixnet.net/blog/post/354459268?utm_source=PIXNET&utm_medium=ppage",
   "https://ivy50819.pixnet.net/blog/post/353592850?utm_source=PIXNET&utm_medium=ppage",
   "https://ivy50819.pixnet.net/blog/post/353530206?utm_source=PIXNET&utm_medium=ppage",
   "https://rainie0516.pixnet.net/blog/post/361081580?utm_source=PIXNET&utm_medium=ppage",
   "https://chunyu405.pixnet.net/blog/post/231307420?utm_source=PIXNET&utm_medium=ppage",
   "https://aaa22034381.pixnet.net/blog/post/40833553?utm_source=PIXNET&utm_medium=ppage",
   "https://even615.pixnet.net/blog/post/121081186?utm_source=PIXNET&utm_medium=ppage",
   "https://kikimilk2.pixnet.net/blog/post/334337296?utm_source=PIXNET&utm_medium=ppage",
   "https://may0708.pixnet.net/blog/post/469300985?utm_source=PIXNET&utm_medium=ppage",
   "https://even615.pixnet.net/blog/post/120822257?utm_source=PIXNET&utm_medium=ppage",
   "https://ivy50819.pixnet.net/blog/post/352610521?utm_source=PIXNET&utm_medium=ppage",
   "https://ivy50819.pixnet.net/blog/post/353118508?utm_source=PIXNET&utm_medium=ppage",
   "https://any3can.pixnet.net/blog/post/351826306?utm_source=PIXNET&utm_medium=ppage",
   "https://ivan1115.pixnet.net/blog/post/45543343?utm_source=PIXNET&utm_medium=ppage"




      # ä½ å¯ä»¥ç¹¼çºŒåŠ å…¥æ›´å¤šç¶²å€
    ]

    start_index = 164  # å¾ articles18.csv é–‹å§‹å‘½å

    for i, url in enumerate(urls, start=start_index):
        try:
            df = scrape_pixnet_article(url)
            filename = f"articles{i}.csv"
            df.to_csv(filename, encoding="utf-8", index=False)
            print(f"âœ… å„²å­˜æˆåŠŸï¼š{filename}")
        except Exception as e:
            print(f"âŒ éŒ¯èª¤ï¼š{url} -> {e}")

import requests
from bs4 import BeautifulSoup
import pandas as pd

# å®šç¾©çˆ¬èŸ²å‡½å¼
def scrape_pixnet_article(url: str) -> pd.DataFrame:
    headers = {
        "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36"
    }
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.content, 'html.parser')

    # æŠ“æ¨™é¡Œ
    title_div = soup.find('div', class_='article')
    title_tag = title_div.find('h2') if title_div else None
    title = title_tag.text.strip() if title_tag else "æ‰¾ä¸åˆ°æ¨™é¡Œ"

    # ç™¼æ–‡æ—¥æœŸ
    publish_li = soup.find('li', class_='publish')
    date = publish_li.get_text(separator=' ', strip=True) if publish_li else "æ‰¾ä¸åˆ°ç™¼æ–‡æ—¥æœŸ"

    # æŠ“å…§å®¹
    content_div = soup.find('div', class_='article-content-inner')
    if content_div:
        p_tags = content_div.find_all('p')
        content = "\n".join(p.text.strip() for p in p_tags)
    else:
        content = "æ‰¾ä¸åˆ°å…§å®¹å€å¡Š"

    return pd.DataFrame([[title, date, content]], columns=["title", "date", "content"])

# ä¸»ç¨‹å¼ï¼šå¤šç¶²å€ã€å¤šCSVæª”æ¡ˆ
if __name__ == "__main__":
    urls = [
   "https://ivan1115.pixnet.net/blog/post/19032551?utm_source=PIXNET&utm_medium=ppage",
   "https://ivan1115.pixnet.net/blog/post/43412341?utm_source=PIXNET&utm_medium=ppage",
   "https://ivan1115.pixnet.net/blog/post/44449504?utm_source=PIXNET&utm_medium=ppage",
   "https://ivan1115.pixnet.net/blog/post/19042661?utm_source=PIXNET&utm_medium=ppage",
   "https://nikitarh.pixnet.net/blog/post/5809681?utm_source=PIXNET&utm_medium=ppage",
   "https://aiting1129.pixnet.net/blog/post/322878578?utm_source=PIXNET&utm_medium=ppage",
   "https://pei95416.pixnet.net/blog/post/360167385?utm_source=PIXNET&utm_medium=ppage",
   "https://pei95416.pixnet.net/blog/post/360611918?utm_source=PIXNET&utm_medium=ppage",
   "https://vivianchang0718.pixnet.net/blog/post/469540721?utm_source=PIXNET&utm_medium=ppage",
   "https://henrychen1974.pixnet.net/blog/post/463671962?utm_source=PIXNET&utm_medium=ppage",
   "https://tctony2222.pixnet.net/blog/post/308301182?utm_source=PIXNET&utm_medium=ppage",
   "https://bbpeng2.pixnet.net/blog/post/228592643?utm_source=PIXNET&utm_medium=ppage",
   "https://bbpeng2.pixnet.net/blog/post/228441404?utm_source=PIXNET&utm_medium=ppage",
   "https://avondiary.pixnet.net/blog/post/223265895?utm_source=PIXNET&utm_medium=ppage",
   "https://lilychen1128.pixnet.net/blog/post/159670262?utm_source=PIXNET&utm_medium=ppage",
   "https://bbpeng2.pixnet.net/blog/post/224178227?utm_source=PIXNET&utm_medium=ppage",
   "https://paicj.pixnet.net/blog/post/400865378?utm_source=PIXNET&utm_medium=ppage",
   "https://evshhips.pixnet.net/blog/post/271898971?utm_source=PIXNET&utm_medium=ppage"




      # ä½ å¯ä»¥ç¹¼çºŒåŠ å…¥æ›´å¤šç¶²å€
    ]

    start_index = 188  # å¾ articles18.csv é–‹å§‹å‘½å

    for i, url in enumerate(urls, start=start_index):
        try:
            df = scrape_pixnet_article(url)
            filename = f"articles{i}.csv"
            df.to_csv(filename, encoding="utf-8", index=False)
            print(f"âœ… å„²å­˜æˆåŠŸï¼š{filename}")
        except Exception as e:
            print(f"âŒ éŒ¯èª¤ï¼š{url} -> {e}")

import requests
from bs4 import BeautifulSoup
import pandas as pd

# å®šç¾©çˆ¬èŸ²å‡½å¼
def scrape_pixnet_article(url: str) -> pd.DataFrame:
    headers = {
        "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36"
    }
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.content, 'html.parser')

    # æŠ“æ¨™é¡Œ
    title_div = soup.find('div', class_='article')
    title_tag = title_div.find('h2') if title_div else None
    title = title_tag.text.strip() if title_tag else "æ‰¾ä¸åˆ°æ¨™é¡Œ"

    # ç™¼æ–‡æ—¥æœŸ
    publish_li = soup.find('li', class_='publish')
    date = publish_li.get_text(separator=' ', strip=True) if publish_li else "æ‰¾ä¸åˆ°ç™¼æ–‡æ—¥æœŸ"

    # æŠ“å…§å®¹
    content_div = soup.find('div', class_='article-content-inner')
    if content_div:
        p_tags = content_div.find_all('p')
        content = "\n".join(p.text.strip() for p in p_tags)
    else:
        content = "æ‰¾ä¸åˆ°å…§å®¹å€å¡Š"

    return pd.DataFrame([[title, date, content]], columns=["title", "date", "content"])

# ä¸»ç¨‹å¼ï¼šå¤šç¶²å€ã€å¤šCSVæª”æ¡ˆ
if __name__ == "__main__":
    urls = [
   "https://a0915900120.pixnet.net/blog/post/366912643?utm_source=PIXNET&utm_medium=ppage",
   "https://paicj.pixnet.net/blog/post/407190357?utm_source=PIXNET&utm_medium=ppage",
   "https://vanessarou.pixnet.net/blog/post/176696575?utm_source=PIXNET&utm_medium=ppage",
   "https://paicj.pixnet.net/blog/post/407161029?utm_source=PIXNET&utm_medium=ppage",
   "https://kikicoco5.pixnet.net/blog/post/122550170?utm_source=PIXNET&utm_medium=ppage",
   "https://paicj.pixnet.net/blog/post/407125781?utm_source=PIXNET&utm_medium=ppage",
   "https://paicj.pixnet.net/blog/post/407089745?utm_source=PIXNET&utm_medium=ppage",
   "https://livi1233.pixnet.net/blog/post/346080981?utm_source=PIXNET&utm_medium=ppage",
   "https://ajhomom.pixnet.net/blog/post/166541563?utm_source=PIXNET&utm_medium=ppage",
   "https://flowermei713.pixnet.net/blog/post/577147672?utm_source=PIXNET&utm_medium=ppage",
   "https://ajhomom.pixnet.net/blog/post/164343472?utm_source=PIXNET&utm_medium=ppage",
   "https://jack74327.pixnet.net/blog/post/71319535?utm_source=PIXNET&utm_medium=ppage",
   "https://paicj.pixnet.net/blog/post/407231213?utm_source=PIXNET&utm_medium=ppage",
   "https://aaa22034381.pixnet.net/blog/post/181447807?utm_source=PIXNET&utm_medium=ppage"





      # ä½ å¯ä»¥ç¹¼çºŒåŠ å…¥æ›´å¤šç¶²å€
    ]

    start_index = 206  # å¾ articles18.csv é–‹å§‹å‘½å

    for i, url in enumerate(urls, start=start_index):
        try:
            df = scrape_pixnet_article(url)
            filename = f"articles{i}.csv"
            df.to_csv(filename, encoding="utf-8", index=False)
            print(f"âœ… å„²å­˜æˆåŠŸï¼š{filename}")
        except Exception as e:
            print(f"âŒ éŒ¯èª¤ï¼š{url} -> {e}")

import requests
from bs4 import BeautifulSoup
import pandas as pd

# å®šç¾©çˆ¬èŸ²å‡½å¼
def scrape_pixnet_article(url: str) -> pd.DataFrame:
    headers = {
        "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36"
    }
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.content, 'html.parser')

    # æŠ“æ¨™é¡Œ
    title_div = soup.find('div', class_='article')
    title_tag = title_div.find('h2') if title_div else None
    title = title_tag.text.strip() if title_tag else "æ‰¾ä¸åˆ°æ¨™é¡Œ"

    # ç™¼æ–‡æ—¥æœŸ
    publish_li = soup.find('li', class_='publish')
    date = publish_li.get_text(separator=' ', strip=True) if publish_li else "æ‰¾ä¸åˆ°ç™¼æ–‡æ—¥æœŸ"

    # æŠ“å…§å®¹
    content_div = soup.find('div', class_='article-content-inner')
    if content_div:
        p_tags = content_div.find_all('p')
        content = "\n".join(p.text.strip() for p in p_tags)
    else:
        content = "æ‰¾ä¸åˆ°å…§å®¹å€å¡Š"

    return pd.DataFrame([[title, date, content]], columns=["title", "date", "content"])

# ä¸»ç¨‹å¼ï¼šå¤šç¶²å€ã€å¤šCSVæª”æ¡ˆ
if __name__ == "__main__":
    urls = [
  "https://paicj.pixnet.net/blog/post/407041145?utm_source=PIXNET&utm_medium=ppage",
  "https://hardaway.com.tw/blog/post/49349192?utm_source=PIXNET&utm_medium=ppage",
  "https://carrieok0925.pixnet.net/blog/post/162353272?utm_source=PIXNET&utm_medium=ppage",
  "https://paicj.pixnet.net/blog/post/407029537?utm_source=PIXNET&utm_medium=ppage",
  "https://evshhips.pixnet.net/blog/post/357607860?utm_source=PIXNET&utm_medium=ppage",
  "https://alrena.pixnet.net/blog/post/234145656?utm_source=PIXNET&utm_medium=ppage",
  "https://evshhips.pixnet.net/blog/post/357498300?utm_source=PIXNET&utm_medium=ppage",
  "https://wtw332589.pixnet.net/blog/post/345925543?utm_source=PIXNET&utm_medium=ppage",
  "https://evshhips.pixnet.net/blog/post/357474357?utm_source=PIXNET&utm_medium=ppage",
  "https://mstravel20.pixnet.net/blog/post/157958401?utm_source=PIXNET&utm_medium=ppage",
  "https://v84454058.pixnet.net/blog/post/357450765?utm_source=PIXNET&utm_medium=ppage#google_vignette",
  "https://evshhips.pixnet.net/blog/post/357431142?utm_source=PIXNET&utm_medium=ppage",
  "https://little15.pixnet.net/blog/post/50869564?utm_source=PIXNET&utm_medium=ppage",
  "https://evshhips.pixnet.net/blog/post/357409563?utm_source=PIXNET&utm_medium=ppage",
  "https://mei30530.pixnet.net/blog/post/576646692?utm_source=PIXNET&utm_medium=ppage",
  "https://monkeybowbi.pixnet.net/blog/post/576648844?utm_source=PIXNET&utm_medium=ppage",
  "https://evonne1205.pixnet.net/blog/post/152523736?utm_source=PIXNET&utm_medium=ppage",
  "https://campingshark86.pixnet.net/blog/post/152292595?utm_source=PIXNET&utm_medium=ppage",
  "https://mei30530.pixnet.net/blog/post/576542712?utm_source=PIXNET&utm_medium=ppage",
  "https://sammi0224.pixnet.net/blog/post/122328713?utm_source=PIXNET&utm_medium=ppage",
  "https://katrina1207.pixnet.net/blog/post/150030244?utm_source=PIXNET&utm_medium=ppage",
  "https://aaa22034381.pixnet.net/blog/post/142558195?utm_source=PIXNET&utm_medium=ppage",
  "https://yunjie059.pixnet.net/blog/post/149460637?utm_source=PIXNET&utm_medium=ppage",
  "https://bellwort9.pixnet.net/blog/post/71167279?utm_source=PIXNET&utm_medium=ppage"






      # ä½ å¯ä»¥ç¹¼çºŒåŠ å…¥æ›´å¤šç¶²å€
    ]

    start_index = 220  # å¾ articles18.csv é–‹å§‹å‘½å

    for i, url in enumerate(urls, start=start_index):
        try:
            df = scrape_pixnet_article(url)
            filename = f"articles{i}.csv"
            df.to_csv(filename, encoding="utf-8", index=False)
            print(f"âœ… å„²å­˜æˆåŠŸï¼š{filename}")
        except Exception as e:
            print(f"âŒ éŒ¯èª¤ï¼š{url} -> {e}")