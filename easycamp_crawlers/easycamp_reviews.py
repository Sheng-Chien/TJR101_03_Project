import requests
from bs4 import BeautifulSoup
import re
import random
import time
from urllib.parse import urljoin
from pathlib import Path
import json

from reviews_links import get_campsite_links,get_city_links,get_review_links


headers = {
        "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36"}

def get_camp_name(soup):
    """ç‡Ÿåœ°åŸºæœ¬è³‡è¨Š""" 
    h1_tag =  soup.select_one("h1")
    name = h1_tag.contents[0].strip()
    return name


def get_overall_stars(soup):
    """ç‡Ÿåœ°ç²å¾—æ˜Ÿæ•¸"""
    all_stars = soup.select_one("a.icon-star-position.assessment_scroll")#å›å‚³ä¸€å€‹tagç‰©ä»¶
    if all_stars:
        given_stars = len(all_stars.select("i.fa-star"))
        return given_stars
    return 0
    
def get_review_count(soup):
    """ç²å¾—è©•è«–æ•¸"""
    review_count = soup.select_one("h5.icon-font-color")
    if review_count:
        text = review_count.text.strip()
        match = re.search(r'([\d,]+)', text) #æŠ“å‡ºç¬¬ä¸€å€‹åŒ…å«æ•¸å­—èˆ‡é€—è™Ÿçš„ç‰‡æ®µ
        if match:
            number_str = match.group(1).replace(",", "")  # ä¸è«–åƒåˆ†ä½æœ‰ç„¡é€—è™Ÿéƒ½å…ˆç§»é™¤
            return int(number_str)
    return 0


def get_score(soup):    
    """ç‡Ÿåœ°å„é …å¾—åˆ†"""
    scores = []
    score_blocks = soup.select("div.col-md-12.col-sm-12.col-xs-12.evaluation-padding div.text-center")
    if not score_blocks:  # å¦‚æœæ²’æœ‰è©•åˆ†å€å¡Šï¼Œè¿”å›äº”å€‹ None é¿å…å¾ŒçºŒå‡ºéŒ¯
        return [None] * 5 
    for block in score_blocks:
        star_count = len(block.select("i.fa-star"))  # è¨ˆç®—æ˜Ÿæ˜Ÿæ•¸
        scores.append(star_count)  
    return scores


def get_customer_name(soup): 
    """è©•è«–è€…å§“å"""
    costumer_name = soup.select_one("div.col-md-12.col-sm-12.col-xs-12 > h3") 
    return re.sub(r'[\s\u200B\u200C\u200D\uFEFF]+', '', costumer_name.text) if costumer_name else None


def get_dates(soup):
    """å…¥ä½æ—¥æœŸ&è©•è«–æ—¥æœŸ"""
    all_divs = soup.select("div.col-md-12.col-sm-12.col-xs-12.font-size-16px")
    checkin_date = None
    review_date = None
    for div in all_divs:
        text = div.text.strip()
        date_match = re.search(r"\d{4}/\d{2}/\d{2}", text)  
        if "å…¥ä½" in text and date_match:
            checkin_date = date_match.group()
        elif "è©•åƒ¹" in text and date_match:
            review_date = date_match.group()
    return checkin_date, review_date


def get_customer_rating(soup):
    """è©•è«–è€…çµ¦äºˆçš„æ˜Ÿæ•¸""" 
    rating_div= soup.select_one("div.col-md-3.col-sm-3.col-xs-3.icon-star-padding")
    customer_rating = len(rating_div.select("i.fa-star"))
    return customer_rating


def get_customer_reviews(block):
    """è©•è«–å…§å®¹""" 
    title_tag = block.select_one("div.title-font-size.english-break-word")
    content_tag = block.select_one("div.content-font-size.english-break-word")    
    review_title = title_tag.text.strip()if title_tag else ""
    review_content = content_tag.text.strip()if content_tag else ""
    return review_title,review_content



def get_one_place_reviews(link, headers):
    """ç²å¾—å–®ä¸€éœ²ç‡Ÿå ´è©•åˆ†"""
    #all_reviews = []
    base_url = "https://www.easycamp.com.tw"
    #for link in urls:
    try:
        response = requests.get(link,headers=headers)
        if response.status_code != 200:
            print(f"è«‹æ±‚å¤±æ•—ï¼Œstatus code: {response.status_code}")
            return None
        soup = BeautifulSoup(response.text, "html.parser")
        camp_name = get_camp_name(soup)
        overall_stars = get_overall_stars(soup)
        review_count = get_review_count(soup)
        traffic, bathroom, view, service, facility = get_score(soup)
        all_reviews = []
        # è©•è«–é ç¿»é 
        current_url = link
        while current_url:
            res = requests.get(current_url, headers=headers)
            if res.status_code != 200:
                print(f"è«‹æ±‚å¤±æ•—ï¼Œstatus code: {res.status_code}")
                break
            soup = BeautifulSoup(res.text, "html.parser")
            review_container = soup.select_one("#tab11")

            if review_container: #å¦‚æœæœ‰è©•è«–å€
                review_blocks = review_container.select("div.row")
                for block in review_blocks:
                    customer_name = get_customer_name(block)
                    checkin_date, review_date = get_dates(block)
                    customer_rating = get_customer_rating(block)
                    review_title, review_content = get_customer_reviews(block)
                    review_data = {
                        "å§“å": customer_name,
                        "å…¥ä½æ—¥æœŸ": checkin_date,
                        "è©•è«–æ—¥æœŸ": review_date,    
                        "è©•åˆ†": customer_rating,
                        "è©•è«–æ¨™é¡Œ": review_title,
                        "è©•è«–å…§å®¹": review_content,    
                    }
                    all_reviews.append(review_data)
                    print(f"æœ‰{len(all_reviews)}ç­†è©•è«–")
            #ä¸‹ä¸€é 
            next_page_link = None
            pagination = soup.select_one("ul.pagination")
            if pagination:
                next_links = pagination.select("li a") 
                for link in next_links:
                    if "ä¸‹ä¸€é " in link.text:
                        href = link.get("href")
                        next_page_link = urljoin(base_url, href)
                        break
            current_url = next_page_link   
            time.sleep(random.uniform(1, 3))          

        #æ”¶é›†ä¸€ç‡Ÿåœ°ä¹‹å®Œæ•´è©•è«–è³‡è¨Š
        rating_data = {
              "ç‡Ÿåœ°åç¨±": camp_name,
              "ç‡Ÿåœ°ç¸½æ˜Ÿç­‰": overall_stars,
              "è©•è«–ç¸½æ•¸": review_count,
              "äº¤é€šä¾¿åˆ©åº¦": traffic,
              "è¡›æµ´æ•´æ½”åº¦": bathroom,
              "æ™¯è§€æ»¿æ„åº¦": view,
              "æœå‹™å“è³ª": service,
              "è¨­æ–½å®Œå–„åº¦":facility,
              "é¡§å®¢è©•è«–":all_reviews
        }
        return rating_data
    
    except Exception as e:
        print(f"æŠ“å– {link} æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        return None


def save_to_json(data, filename):
    """å­˜å…¥ JSON æª”"""
    with open(filename, "w", encoding="utf-8") as f:
       json.dump(data, f, indent=4, ensure_ascii=False)



def main():
    url = "https://www.easycamp.com.tw/store/store_list"  # base.py ä¸­çš„ç¶²å€
    city_links = get_city_links(url, headers)
    campsite_links = get_campsite_links(city_links, headers)
    review_links = get_review_links(campsite_links, headers)

    # ä½¿ç”¨ pathlib å®šç¾© checkpoint æª”æ¡ˆè·¯å¾‘
    checkpoint_file = Path("easycamp_reviews.json")
    all_data = []
     # è‹¥å·²æœ‰ checkpointï¼Œå…ˆè¼‰å…¥å·²ç¶“å„²å­˜çš„è³‡æ–™
    if checkpoint_file.exists():
        with open(checkpoint_file, "r", encoding="utf-8") as f:
            all_data = json.load(f)
   

    processed_names = {camp["ç‡Ÿåœ°åç¨±"] for camp in all_data}
    for i, link in enumerate(review_links):
        print(f"\nğŸ” æ­£åœ¨è™•ç†ç¬¬ {i+1} ç­†ï¼Œå…± {len(review_links)} ç­†")
    #for link in review_links:
        # åˆ¤æ–·æ˜¯å¦å·²ç¶“è™•ç†éé€™å€‹éœ²ç‡Ÿå ´
        camp_preview = get_camp_name(BeautifulSoup(requests.get(link, headers=headers).text, "html.parser"))
        if camp_preview in processed_names:
            print(f"âœ… å·²è™•ç†é {camp_preview}ï¼Œç•¥é")
            continue

        camp_data = get_one_place_reviews(link, headers)
        if camp_data:
            all_data.append(camp_data)

            # å³æ™‚å¯«å…¥ checkpoint
            with open(checkpoint_file, "w", encoding="utf-8") as f:
                json.dump(all_data, f, indent=4, ensure_ascii=False)
            print(f"å„²å­˜ {camp_data['ç‡Ÿåœ°åç¨±']} æˆåŠŸ")

    # review_data = get_one_place_reviews(review_links, headers)
    #save_to_json(review_data, "easycamp_reviews.json")

if __name__ == "__main__":
    main()




